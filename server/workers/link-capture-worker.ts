/**
 * WORKER ASS√çNCRONO - Sistema de Jobs para Deep Crawling
 * ‚úÖ SUPORTA: pause/resume/cancel em tempo real
 * ‚úÖ REPORTA: progress incremental por p√°gina
 */

import { db } from "../db";
import { linkCaptureJobs } from "../../shared/schema";
import { eq, inArray } from "drizzle-orm";
import { DeepCrawler } from "../learn/deep-crawler";
import { WebsiteCrawlerService } from "../learn/website-crawler-service";

class LinkCaptureWorker {
  private isRunning = false;
  private currentJobId: number | null = null;

  async start() {
    if (this.isRunning) return;
    this.isRunning = true;
    console.log("[Worker] üöÄ Link capture worker iniciado");

    while (this.isRunning) {
      try {
        await this.processNextJob();
      } catch (error: any) {
        console.error("[Worker] ‚ùå Erro:", error.message);
      }
      await new Promise(resolve => setTimeout(resolve, 2000));
    }
  }

  stop() {
    this.isRunning = false;
  }

  private async processNextJob() {
    // Pega jobs pending OU running (para permitir resume)
    const [job] = await db
      .select()
      .from(linkCaptureJobs)
      .where(inArray(linkCaptureJobs.status, ["pending", "running"]))
      .limit(1);

    if (!job) return;

    this.currentJobId = job.id;

    // ‚úÖ Refer√™ncia ao crawler (fora do try/catch para acesso no catch)
    let currentCrawler: DeepCrawler | null = null;

    try {
      // Se job estava pending, marca como running
      if (job.status === "pending") {
        await db.update(linkCaptureJobs).set({ 
          status: "running", 
          startedAt: new Date(),
          currentItem: "Iniciando..."
        }).where(eq(linkCaptureJobs.id, job.id));
        console.log(`[Worker] üì• Job ${job.id}: ${job.url}`);
      }

      const metadata = job.metadata as any || {};
      const crawler = new DeepCrawler(job.url, {
        maxDepth: metadata.maxDepth || 5,
        maxPages: metadata.maxPages || 100,
        includeImages: metadata.includeImages !== false
      });

      // Guarda refer√™ncia para uso no catch
      currentCrawler = crawler;

      // ‚úÖ CALLBACK: progress incremental
      crawler.onProgress = async (processed: number, total: number, currentUrl: string) => {
        const updatedJob = await this.getJobStatus(job.id);
        
        // ‚úÖ CHECK: se job foi pausado/cancelado
        if (updatedJob?.status === "paused") {
          console.log(`[Worker] ‚è∏Ô∏è  Job ${job.id} pausado pelo usu√°rio`);
          throw new Error("PAUSED");
        }
        if (updatedJob?.status === "cancelled") {
          console.log(`[Worker] ‚ùå Job ${job.id} cancelado pelo usu√°rio`);
          throw new Error("CANCELLED");
        }

        // ‚úÖ UPDATE: progress em tempo real
        const progress = total > 0 ? Math.floor((processed / total) * 100) : 0;
        await db.update(linkCaptureJobs).set({
          processedItems: processed,
          totalItems: total,
          progress: Math.min(progress, 99), // Reserva 100% para conclus√£o
          currentItem: currentUrl
        }).where(eq(linkCaptureJobs.id, job.id));
      };

      // ‚úÖ CRAWL: com callbacks de progress/cancellation
      const pages = await crawler.crawl();

      // ‚úÖ UPDATE FINAL: garante que processedItems/totalItems refletem progresso real
      await db.update(linkCaptureJobs).set({
        processedItems: pages.length,
        totalItems: pages.length,
        progress: 90
      }).where(eq(linkCaptureJobs.id, job.id));

      // Checa status final antes de enviar para curadoria
      const finalJob = await this.getJobStatus(job.id);
      if (finalJob?.status === "cancelled") {
        await db.update(linkCaptureJobs).set({
          status: "cancelled",
          processedItems: pages.length,
          totalItems: pages.length,
          completedAt: new Date()
        }).where(eq(linkCaptureJobs.id, job.id));
        return;
      }

      // Envia consolidado para curadoria
      await db.update(linkCaptureJobs).set({
        currentItem: `Enviando ${pages.length} p√°ginas para curadoria...`,
        progress: 95
      }).where(eq(linkCaptureJobs.id, job.id));

      const crawlerService = new WebsiteCrawlerService();
      await crawlerService.sendConsolidatedToCuration(pages, metadata.namespace || 'kb/web', job.url);

      // Completa job
      await db.update(linkCaptureJobs).set({
        status: "completed",
        progress: 100,
        processedItems: pages.length,
        totalItems: pages.length,
        currentItem: `‚úì ${pages.length} p√°ginas enviadas para curadoria`,
        completedAt: new Date()
      }).where(eq(linkCaptureJobs.id, job.id));

      console.log(`[Worker] ‚úÖ Job ${job.id} conclu√≠do`);

    } catch (error: any) {
      // ‚úÖ Pega p√°ginas processadas at√© agora (do crawler original)
      const currentProgress = currentCrawler?.getStats().totalPages || 0;

      // Se foi pausado, mant√©m status paused com progresso atual
      if (error.message === "PAUSED") {
        await db.update(linkCaptureJobs).set({
          currentItem: "‚è∏Ô∏è Pausado pelo usu√°rio",
          processedItems: currentProgress,
          totalItems: currentProgress
        }).where(eq(linkCaptureJobs.id, job.id));
        return;
      }

      // Se foi cancelado, marca como cancelled com progresso atual
      if (error.message === "CANCELLED") {
        await db.update(linkCaptureJobs).set({
          status: "cancelled",
          currentItem: "‚ùå Cancelado pelo usu√°rio",
          processedItems: currentProgress,
          totalItems: currentProgress,
          completedAt: new Date()
        }).where(eq(linkCaptureJobs.id, job.id));
        return;
      }

      // Outros erros marcam como failed com progresso atual
      await db.update(linkCaptureJobs).set({
        status: "failed",
        errorMessage: error.message,
        processedItems: currentProgress,
        totalItems: currentProgress,
        completedAt: new Date()
      }).where(eq(linkCaptureJobs.id, job.id));
    }
  }

  private async getJobStatus(jobId: number) {
    const [job] = await db.select().from(linkCaptureJobs).where(eq(linkCaptureJobs.id, jobId)).limit(1);
    return job || null;
  }
}

export const linkCaptureWorker = new LinkCaptureWorker();
linkCaptureWorker.start();
