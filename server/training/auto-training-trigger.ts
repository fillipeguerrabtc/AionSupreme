/**
 * AUTO-TRAINING TRIGGER - Disparo AutomÃ¡tico de Treino
 * 
 * Monitora o sistema e dispara treino automaticamente quando:
 * 1. HÃ¡ X novos exemplos acumulados (default: 100)
 * 2. HÃ¡ pelo menos 1 GPU online disponÃ­vel
 * 3. NÃ£o hÃ¡ treino em andamento
 * 
 * FLUXO:
 * 1. Background job verifica a cada 30 min
 * 2. Se condiÃ§Ãµes OK â†’ gera dataset + dispara treino
 * 3. Distribui jobs entre GPUs disponÃ­veis
 */

import { datasetGenerator } from "./dataset-generator";
import { GPUPool } from "../gpu/pool";
import { db } from "../db";
import { trainingJobs } from "../../shared/schema";
import { eq, desc } from "drizzle-orm";
import { datasetSplitter } from "../federated/dataset-splitter";
import { gradientAggregator } from "../federated/gradient-aggregator";
import { getMetaLearningConfig } from "./meta-learning-config";

interface TrainingConfig {
  model: string;
  lora: {
    r: number;
    alpha: number;
    dropout: number;
  };
  training: {
    epochs: number;
    batchSize: number;
    learningRate: number;
  };
}

export class AutoTrainingTrigger {
  private checkIntervalMs = 30 * 60 * 1000; // 30 minutos
  private intervalId: NodeJS.Timeout | null = null;
  private enabled = true;

  /**
   * Inicia monitoramento automÃ¡tico
   */
  start(): void {
    if (this.intervalId) {
      console.log("[AutoTrain] JÃ¡ estÃ¡ rodando");
      return;
    }

    console.log(`[AutoTrain] âœ… Iniciado - verificando a cada ${this.checkIntervalMs / 1000 / 60} min`);

    // Verificar imediatamente
    this.checkAndTrigger();

    // Depois verificar periodicamente
    this.intervalId = setInterval(() => {
      this.checkAndTrigger();
    }, this.checkIntervalMs);
  }

  /**
   * Para monitoramento
   */
  stop(): void {
    if (this.intervalId) {
      clearInterval(this.intervalId);
      this.intervalId = null;
      console.log("[AutoTrain] â¹ Parado");
    }
  }

  /**
   * Verifica condiÃ§Ãµes e dispara treino se necessÃ¡rio
   * 
   * ENTERPRISE DIAMOND PLUS FEATURES:
   * - Adaptive thresholds based on environment
   * - Differential Privacy budget tracking
   * - Quality gates enforcement
   */
  private async checkAndTrigger(): Promise<void> {
    if (!this.enabled) {
      return;
    }

    console.log("\nğŸ¤– [AutoTrain] Verificando condiÃ§Ãµes para auto-treino...");
    
    // Load adaptive configuration
    const config = getMetaLearningConfig();
    const threshold = config.thresholds.minKBItems; // âœ… BLOCKER #2 FIX: Use minKBItems

    try {
      // âœ… BLOCKER #2 FIX: Usar checkPendingKBItems() em vez de checkPendingExamples()
      // Conta APENAS KB items (documentos aprovados), nÃ£o conversas
      const pendingKBItems = await datasetGenerator.checkPendingKBItems();
      console.log(`   ğŸ“š KB items pendentes: ${pendingKBItems}`);
      console.log(`   ğŸ¯ Threshold (modo ${config.mode}): ${threshold} KB items`);

      if (pendingKBItems < threshold) {
        console.log(`   âš  Insuficiente - precisa de ${threshold} KB items`);
        return;
      }

      console.log(`   âœ… Threshold atingido! (${pendingKBItems} >= ${threshold} KB items)`);

      // CONDIÃ‡ÃƒO 2: Verificar GPUs disponÃ­veis
      const onlineWorkers = await GPUPool.getOnlineWorkers();
      
      if (onlineWorkers.length === 0) {
        console.log("   âš  Nenhuma GPU online - aguardando workers");
        return;
      }

      console.log(`   âœ… ${onlineWorkers.length} GPU(s) online disponÃ­vel(is)`);

      // CONDIÃ‡ÃƒO 3: Verificar se jÃ¡ hÃ¡ treino em andamento
      const runningJobs = await db.query.trainingJobs.findMany({
        where: eq(trainingJobs.status, "running"),
      });

      if (runningJobs.length > 0) {
        console.log(`   âš  ${runningJobs.length} job(s) em andamento - aguardando conclusÃ£o`);
        return;
      }

      console.log("   âœ… Nenhum treino em andamento");

      // TODAS AS CONDIÃ‡Ã•ES OK! ğŸš€
      console.log("\n   ğŸ¯ TODAS CONDIÃ‡Ã•ES OK - INICIANDO AUTO-TREINO!");

      await this.triggerTraining(config);
    } catch (error: any) {
      console.error(`[AutoTrain] âŒ Erro no check:`, error.message);
    }
  }

  /**
   * Dispara treino automaticamente
   */
  private async triggerTraining(config: ReturnType<typeof getMetaLearningConfig>): Promise<void> {
    try {
      // STEP 1: Gerar dataset automaticamente
      console.log("\n   ğŸ“¦ [1/3] Gerando dataset...");
      const dataset = await datasetGenerator.generateAutoDataset();

      if (!dataset) {
        console.log("   âŒ Falha ao gerar dataset");
        return;
      }

      console.log(`   âœ… Dataset gerado: ${dataset.examplesCount} exemplos`);

      // STEP 2: Criar job de treino com LoRA + Privacy Heuristics
      console.log("\n   ğŸ”§ [2/3] Criando job de treino...");
      
      const [job] = await db.insert(trainingJobs).values({
        name: `Auto-Training ${config.mode} ${new Date().toISOString()}`,
        description: `Treino automÃ¡tico com ${dataset.examplesCount} exemplos (modo: ${config.mode})`,
        model: "llama3-8b", // Default model
        datasetId: dataset.datasetId,
        status: "pending",
        config: {
          // LoRA configuration (parameter-efficient fine-tuning)
          lora: {
            r: config.lora.rank,
            alpha: config.lora.alpha,
            dropout: config.lora.dropout,
            targetModules: config.lora.targetModules,
          },
          // Training parameters
          training: {
            epochs: config.training.epochs,
            batchSize: config.training.batchSize,
            learningRate: config.training.learningRate,
            warmupSteps: config.training.warmupSteps,
            gradientAccumulationSteps: config.training.gradientAccumulationSteps,
          },
          // Privacy Heuristics (threshold + LoRA + replay + PII)
          privacy: {
            mode: 'heuristics',
            threshold: config.thresholds.minExamples,
            piiRedaction: config.piiRedaction.enabled,
            replayBuffer: config.replayBuffer.enabled,
            loraRank: config.lora.rank,
          },
          // Meta info
          autoTriggered: true,
          mode: config.mode,
          replayBufferEnabled: config.replayBuffer.enabled,
        },
      } as any).returning();

      console.log(`   âœ… Job criado: ID ${job.id}`);
      console.log(`   ğŸ” Privacy: Heuristics (threshold=${config.thresholds.minExamples}, LoRA=${config.lora.rank}, PII=${config.piiRedaction.enabled})`);

      // STEP 3: Verificar quantas GPUs disponÃ­veis
      console.log("\n   ğŸ® [3/5] Verificando GPUs disponÃ­veis...");
      const availableWorkers = await GPUPool.getAvailableWorkersForTraining();
      
      if (availableWorkers.length === 0) {
        console.log("   âŒ Nenhuma GPU disponÃ­vel");
        return;
      }

      console.log(`   âœ… ${availableWorkers.length} GPU(s) disponÃ­vel(is)`);

      // STEP 4: Distribuir treino (Federated ou Single)
      const federatedThreshold = config.thresholds.federatedMinimum;
      if (availableWorkers.length > 1 && dataset.examplesCount >= federatedThreshold) {
        // FEDERATED LEARNING - MÃºltiplas GPUs
        console.log("\n   ğŸŒ [4/5] MODO FEDERADO - Dividindo dataset...");
        
        // Buscar dataset real do banco para pegar storagePath
        const datasetRecord = await db.query.datasets.findFirst({
          where: eq((await import("../../shared/schema")).datasets.id, dataset.datasetId),
        });

        if (!datasetRecord?.storagePath) {
          console.log("   âŒ Dataset storagePath nÃ£o encontrado");
          return;
        }

        // Dividir dataset em chunks (1 chunk por GPU)
        const splitResult = await datasetSplitter.splitDataset(
          datasetRecord.storagePath,
          availableWorkers.length,
          job.id
        );

        console.log(`   âœ… Dataset dividido em ${splitResult.totalChunks} chunks`);
        console.log(`   ğŸ“Š ~${splitResult.avgChunkSize} exemplos por GPU`);

        // STEP 5: Iniciar treino em TODAS as GPUs em paralelo
        console.log("\n   ğŸš€ [5/5] Iniciando treino DISTRIBUÃDO...");

        // Construir URL base pÃºblica (com fallback para desenvolvimento local)
        let baseUrl: string;
        if (process.env.PUBLIC_BASE_URL) {
          // Prioridade 1: Env var configurÃ¡vel
          baseUrl = process.env.PUBLIC_BASE_URL;
        } else if (process.env.REPLIT_DEV_DOMAIN) {
          // Prioridade 2: Replit deployment/dev
          baseUrl = `https://${process.env.REPLIT_DEV_DOMAIN}`;
        } else if (process.env.REPL_SLUG && process.env.REPL_OWNER) {
          // Prioridade 3: Replit formato antigo
          baseUrl = `https://${process.env.REPL_SLUG}.${process.env.REPL_OWNER}.repl.co`;
        } else {
          // Fallback: localhost (desenvolvimento local)
          const port = process.env.PORT || '5000';
          baseUrl = `http://localhost:${port}`;
          console.log(`   âš ï¸  Usando localhost - workers remotos NÃƒO conseguirÃ£o baixar chunks!`);
          console.log(`   ğŸ’¡ Configure PUBLIC_BASE_URL env var com sua URL pÃºblica`);
        }

        const trainingPromises = splitResult.chunks.map(async (chunk, idx) => {
          const worker = availableWorkers[idx];
          
          // âœ… CORREÃ‡ÃƒO: Passar URL downloadable em vez de file path local
          const chunkUrl = `${baseUrl}/api/datasets/chunks/${job.id}/${chunk.chunkIndex}/download`;
          
          return GPUPool.startTraining(worker.id, job.id, {
            datasetPath: chunkUrl, // URL que workers remotos podem baixar
            modelName: "llama3-8b",
            loraConfig: {
              r: config.lora.rank,
              alpha: config.lora.alpha,
              dropout: config.lora.dropout,
            },
            trainingArgs: {
              epochs: config.training.epochs,
              batchSize: config.training.batchSize,
              learningRate: config.training.learningRate,
            },
          });
        });

        const results = await Promise.all(trainingPromises);
        const successCount = results.filter(r => r === true).length;

        console.log("\n   ğŸ‰ TREINO FEDERADO INICIADO!");
        console.log(`   ğŸ“Š Dataset: ${dataset.examplesCount} exemplos`);
        console.log(`   ğŸ® GPUs ativas: ${successCount}/${availableWorkers.length}`);
        console.log(`   ğŸŒ Modo: FEDERATED LEARNING (FedAvg)`);
        console.log(`   ğŸ“ Job ID: ${job.id}`);
        
        console.log("\n   ğŸ¤– AUTOMAÃ‡ÃƒO 100% ATIVA:");
        console.log("   âœ… GradientAggregationCoordinator monitorando (check: 30s)");
        console.log("   âœ… FedAvg automÃ¡tico quando todos workers completarem");
        console.log("   âœ… Deployment automÃ¡tico do modelo (check: 1min)");
        console.log("   âœ… Hot reload automÃ¡tico nos workers (zero downtime)");
      } else {
        // SINGLE GPU - Treino tradicional
        console.log("\n   ğŸ’» [4/5] MODO SINGLE-GPU...");
        
        const worker = availableWorkers[0];
        
        await GPUPool.startTraining(worker.id, job.id, {
          datasetPath: String(dataset.datasetId),
          modelName: "llama3-8b",
          loraConfig: {
            r: config.lora.rank,
            alpha: config.lora.alpha,
            dropout: config.lora.dropout,
          },
          trainingArgs: {
            epochs: config.training.epochs,
            batchSize: config.training.batchSize,
            learningRate: config.training.learningRate,
          },
        });

        console.log("\n   ğŸš€ AUTO-TREINO INICIADO!");
        console.log(`   ğŸ“Š Dataset: ${dataset.examplesCount} exemplos`);
        console.log(`   ğŸ® GPU: Worker #${worker.id} (${worker.provider})`);
        console.log(`   ğŸ“ Job ID: ${job.id}`);
      }
    } catch (error: any) {
      console.error(`[AutoTrain] âŒ Erro ao disparar treino:`, error.message);
    }
  }

  /**
   * Disparo manual (para testes)
   */
  async triggerNow(): Promise<void> {
    console.log("\nğŸš€ [AutoTrain] Disparo MANUAL iniciado...");
    const config = getMetaLearningConfig();
    await this.triggerTraining(config);
  }

  /**
   * Ativar/desativar
   */
  setEnabled(enabled: boolean): void {
    this.enabled = enabled;
    console.log(`[AutoTrain] ${enabled ? "âœ… Ativado" : "âŒ Desativado"}`);
  }
}

// Export singleton
export const autoTrainingTrigger = new AutoTrainingTrigger();
