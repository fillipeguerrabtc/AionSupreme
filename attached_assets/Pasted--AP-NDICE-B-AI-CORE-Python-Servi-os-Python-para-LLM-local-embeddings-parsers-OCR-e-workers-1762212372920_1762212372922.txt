üêç AP√äNDICE B ‚Äî AI CORE (Python)

Servi√ßos Python para LLM local, embeddings, parsers/OCR, e workers GPU (Colab/Kaggle). Todos exp√µem HTTP para o orquestrador Node/Express.

üìÅ Estrutura (Python)
python/
 ‚îú‚îÄ serve/
 ‚îÇ   ‚îú‚îÄ model_api.py          # gera√ß√£o e streaming
 ‚îÇ   ‚îú‚îÄ embed_api.py          # embeddings
 ‚îÇ   ‚îú‚îÄ health.py             # /health
 ‚îú‚îÄ llm/
 ‚îÇ   ‚îú‚îÄ loader.py             # carrega modelo (INT4/LoRA)
 ‚îÇ   ‚îú‚îÄ generate.py           # gera√ß√£o
 ‚îÇ   ‚îú‚îÄ stream.py             # SSE de tokens
 ‚îú‚îÄ rag/
 ‚îÇ   ‚îú‚îÄ embedder.py           # encoder de embeddings
 ‚îú‚îÄ ingest/
 ‚îÇ   ‚îú‚îÄ parsers.py            # pdf/docx/csv/html
 ‚îÇ   ‚îú‚îÄ ocr.py                # Tesseract/Images
 ‚îú‚îÄ utils/
 ‚îÇ   ‚îú‚îÄ storage.py            # checkpoints no Drive/S3
 ‚îÇ   ‚îú‚îÄ logging.py            # logger Python
 ‚îú‚îÄ requirements.txt

üß© B1. python/requirements.txt
fastapi
uvicorn
pydantic
torch
transformers
accelerate
bitsandbytes
scipy
numpy
faiss-cpu
sentence-transformers
pytesseract
pdfminer.six
python-docx
pandas
beautifulsoup4
lxml
opencv-python
httpx

üß© B2. python/utils/logging.py
import logging, sys
def setup_logger(name="aion.py", level=logging.INFO):
    logger = logging.getLogger(name)
    logger.setLevel(level)
    h = logging.StreamHandler(sys.stdout)
    h.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))
    if not logger.handlers: logger.addHandler(h)
    return logger
log = setup_logger()

üß© B3. python/llm/loader.py
import os, torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

MODEL_ID = os.getenv("AION_MODEL_ID", "meta-llama/Llama-3.1-8B-Instruct")
USE_INT4 = os.getenv("AION_INT4", "1") == "1"
LORA_PATH = os.getenv("AION_LORA_PATH")  # opcional

def load_model():
    quant = BitsAndBytesConfig(load_in_4bit=USE_INT4, bnb_4bit_compute_dtype=torch.bfloat16)
    tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=quant,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    # opcional: carregar LoRA
    if LORA_PATH and os.path.exists(LORA_PATH):
        from peft import PeftModel
        model = PeftModel.from_pretrained(model, LORA_PATH)
    model.eval()
    return tok, model

üß© B4. python/llm/generate.py
import torch
from transformers import TextIteratorStreamer
from typing import Generator

def generate(model, tok, prompt: str, temperature=0.7, top_p=0.9, max_new_tokens=512):
    inputs = tok(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(
            **inputs,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            max_new_tokens=max_new_tokens,
            pad_token_id=tok.eos_token_id
        )
    return tok.decode(out[0], skip_special_tokens=True)

def stream(model, tok, prompt: str, temperature=0.7, top_p=0.9, max_new_tokens=512) -> Generator[str, None, None]:
    inputs = tok(prompt, return_tensors="pt").to(model.device)
    streamer = TextIteratorStreamer(tok, skip_prompt=True, skip_special_tokens=True)
    gen_kwargs = dict(
        **inputs, streamer=streamer, do_sample=True, temperature=temperature,
        top_p=top_p, max_new_tokens=max_new_tokens, pad_token_id=tok.eos_token_id
    )
    import threading
    t = threading.Thread(target=model.generate, kwargs=gen_kwargs)
    t.start()
    for token in streamer:
        yield token

üß© B5. python/rag/embedder.py
import os
from sentence_transformers import SentenceTransformer
MODEL = os.getenv("AION_EMBED_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
_model = SentenceTransformer(MODEL)

def embed_texts(texts):
    return _model.encode(texts, normalize_embeddings=True).tolist()

def embed(text: str):
    return embed_texts([text])[0]

üß© B6. python/ingest/parsers.py
import io, pandas as pd
from bs4 import BeautifulSoup
from pdfminer.high_level import extract_text
from docx import Document

def parse_pdf(bytes_blob: bytes) -> str:
    return extract_text(io.BytesIO(bytes_blob))

def parse_docx(bytes_blob: bytes) -> str:
    f = io.BytesIO(bytes_blob)
    doc = Document(f)
    return "\n".join(p.text for p in doc.paragraphs)

def parse_csv(bytes_blob: bytes) -> str:
    df = pd.read_csv(io.BytesIO(bytes_blob))
    return df.to_csv(index=False)

def parse_html(html: str) -> str:
    soup = BeautifulSoup(html, "lxml")
    for s in soup(["script","style"]): s.decompose()
    return " ".join(soup.get_text(" ").split())

üß© B7. python/ingest/ocr.py
import cv2, pytesseract, numpy as np

def ocr_image(bytes_blob: bytes) -> str:
    arr = np.frombuffer(bytes_blob, np.uint8)
    img = cv2.imdecode(arr, cv2.IMREAD_GRAYSCALE)
    img = cv2.threshold(img, 0, 255, cv2.THRESH_OTSU)[1]
    return pytesseract.image_to_string(img, lang="por+eng")

# OCR para PDF escaneado: extrair p√°ginas como imagens (ex.: pdf2image) e concatenar textos.

üß© B8. python/serve/model_api.py (FastAPI: run + stream)
import os
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
from utils.logging import log
from llm.loader import load_model
from llm.generate import generate, stream

app = FastAPI(title="AION Model API")
tok, model = load_model()

@app.get("/health")
def health(): return {"ok": True}

@app.post("/generate")
async def generate_route(req: Request):
    body = await req.json()
    text = body.get("prompt") or body.get("input")
    if not text: return JSONResponse({"error":"prompt-required"}, 400)
    out = generate(model, tok, text, body.get("temperature",0.7), body.get("top_p",0.9), body.get("max_new_tokens",512))
    return {"ok": True, "output": out}

@app.get("/stream")
async def stream_route(q: str):
    def iter():
        for t in stream(model, tok, q):
            yield t
    return StreamingResponse(iter(), media_type="text/plain")

üß© B9. python/serve/embed_api.py
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from rag.embedder import embed, embed_texts

app = FastAPI(title="AION Embed API")

@app.get("/health")
def h(): return {"ok": True}

@app.post("/embed")
async def emb(req: Request):
    b = await req.json()
    if isinstance(b, list): return {"vectors": embed_texts(b)}
    if "text" in b: return {"vector": embed(b["text"])}
    return JSONResponse({"error":"bad-payload"}, 400)

üñ•Ô∏è AP√äNDICE C ‚Äî FRONTEND (React + SSE + Sanitiza√ß√£o + Painel)

SSE no chat, sanitiza√ß√£o global e telas de Curadoria / KB / M√©tricas / Policy.

üìÅ Estrutura (client)
client/
 ‚îú‚îÄ src/
 ‚îÇ  ‚îú‚îÄ pages/
 ‚îÇ  ‚îÇ  ‚îú‚îÄ chat/ChatPage.tsx
 ‚îÇ  ‚îÇ  ‚îú‚îÄ admin/KnowledgePage.tsx
 ‚îÇ  ‚îÇ  ‚îú‚îÄ admin/CurationPage.tsx
 ‚îÇ  ‚îÇ  ‚îú‚îÄ admin/PolicyPage.tsx
 ‚îÇ  ‚îÇ  ‚îú‚îÄ admin/MetricsPage.tsx
 ‚îÇ  ‚îú‚îÄ components/ui/...
 ‚îÇ  ‚îú‚îÄ lib/stream.ts
 ‚îÇ  ‚îú‚îÄ lib/sanitize.ts

üß© C1. client/src/lib/stream.ts
export function streamAnswer(payload: any, onToken: (t:string)=>void, onDone: ()=>void, onError:(m:string)=>void){
  const es = new EventSource(`/api/chat/stream?payload=${encodeURIComponent(JSON.stringify(payload))}`);
  es.onmessage = (e) => {
    if (e.data === "[DONE]") { es.close(); onDone(); return; }
    try { const { token } = JSON.parse(e.data); onToken(token); } catch {}
  };
  es.addEventListener("error", () => { es.close(); onError("stream-error"); });
}

üß© C2. client/src/lib/sanitize.ts
import DOMPurify from "dompurify";
export const sanitize = (html: string) => DOMPurify.sanitize(html);

üß© C3. client/src/pages/chat/ChatPage.tsx (uso do SSE)
import { useState } from "react";
import { streamAnswer } from "../../lib/stream";

export default function ChatPage(){
  const [sessionId] = useState(()=> crypto.randomUUID());
  const [input, setInput] = useState("");
  const [out, setOut] = useState("");
  const [busy, setBusy] = useState(false);

  const send = () => {
    setBusy(true); setOut("");
    streamAnswer({ sessionId, input },
      (tok) => setOut(prev => prev + tok),
      () => setBusy(false),
      (_err) => setBusy(false)
    );
  };

  return (
    <div className="p-6">
      <textarea value={input} onChange={e=>setInput(e.target.value)} className="w-full border p-2" rows={3}/>
      <button onClick={send} disabled={busy} className="mt-2 px-4 py-2 bg-black text-white rounded">{busy?"Gerando...":"Enviar"}</button>
      <pre className="mt-4 whitespace-pre-wrap">{out}</pre>
    </div>
  );
}

üß© C4. P√°ginas Admin (esqueleto m√≠nimo, conecte √†s rotas A24/A23/A18/A16)

KnowledgePage.tsx

Lista itens aprovados, bot√µes de reindexar/remover.
CurationPage.tsx

Lista /curation/pending, bot√µes aprovar/rejeitar (POST /curation/action).
PolicyPage.tsx

Exibe preset atual (/policy) e atualiza (/policy/update).
MetricsPage.tsx

Chama /metrics e desenha KPI/heatmaps (pode usar chart.js).

Para XSS: onde usar HTML, aplique sanitize().

‚òÅÔ∏è AP√äNDICE D ‚Äî ORQUESTRA√á√ÉO & DEPLOY (Colab/Kaggle/GCP/Replit)
üß© D1. Notebooks Colab/Kaggle (pseudocode)

Montar Drive

git clone repo

pip install -r python/requirements.txt

Subir Model API e Embed API:

!python -m uvicorn serve.model_api:app --host 0.0.0.0 --port 7001 & 
!python -m uvicorn serve.embed_api:app --host 0.0.0.0 --port 7005 &


Expor com ngrok/localtunnel e exportar LOCAL_MODEL_URL, LOCAL_MODEL_STREAM_URL, EMBED_URL.

üß© D2. Watchdog (GCP e2-micro) ‚Äî orchestrator/watchdog.py
import time, requests, os

ENDPOINTS = [
  os.getenv("LOCAL_MODEL_URL","http://localhost:7001/health"),
]
def alive(url):
  try: return requests.get(url, timeout=5).ok
  except: return False

while True:
  for ep in ENDPOINTS:
    if not alive(ep):
      # trigger notebooks via Apps Script / Colab API / Selenium headless
      print("Reiniciar worker:", ep)
  time.sleep(60)

üß© D3. docker/Dockerfile (backend Node)
FROM node:20-alpine
WORKDIR /app
COPY server ./server
COPY client ./client
RUN cd server && npm i && cd ../client && npm i && npm run build
RUN addgroup -S app && adduser -S app -G app && chown -R app:app /app
USER app
CMD ["node", "server/index.js"]


Ajuste para TypeScript (ts-node/tsc) conforme seu projeto.

üß© D4. .env.example (consolidado)
DATABASE_URL=
SESSION_SECRET=
OPENROUTER_API_KEY=
GROQ_API_KEY=
GOOGLE_API_KEY=
HF_API_KEY=
GPU_WORKERS=http://<colab1>,http://<kaggle1>
LOCAL_MODEL_URL=http://localhost:7001
LOCAL_MODEL_STREAM_URL=http://localhost:7001/stream
EMBED_URL=http://localhost:7005/embed
VECTOR_SNAPSHOT_PATH=./data/vectorstore.snapshot.json
CORS_ORIGIN=http://localhost:3000
LOG_LEVEL=info
APP_VERSION=1.0.0

üìà AP√äNDICE E ‚Äî M√âTRICAS (Prometheus/Grafana)

Sem depend√™ncia obrigat√≥ria, mas recomend√°vel.

üß© E1. Export Prometheus (texto)

Adapte metrics/exporter.ts para endpoint text/plain:

app.get("/metrics/prom", async (_req, res) => {
  res.setHeader("Content-Type", "text/plain");
  res.send(await metricsExporter.prom());
});

üß© E2. Principais KPIs

nDCG@k e MRR (qualidade do RAG)

CTR (click-through rate de respostas com sugest√µes)

CR (conversion rate das a√ß√µes do agente vendedor)

fallback_rate (% de chamadas que foram para provedores externos)

lat√™ncia m√©dia por camada (LLM local, GPU worker, free provider, OpenAI)

Voc√™ pode agregar no backend e expor no /metrics para consumo do dashboard.

üîê AP√äNDICE F ‚Äî SEGURAN√áA

Helmet + CORS + Rate Limit (j√° entregues)

Auth JWT (separar rotas /admin)

Sanitiza√ß√£o em todo lugar que renderiza HTML (j√° entregue)

Auditoria (policy-audit + addAudit)

Secrets: nunca logar valores; apenas presen√ßa.

Docker rootless (j√° entregue)

‚úÖ AP√äNDICE G ‚Äî TEST PLAN (QA)
G1. Unit

Embeddings retornam vetores normalizados.

VectorStore: addVector, similaritySearch, save e load.

Orquestrador: simula indisponibilidade local ‚Üí GPU ‚Üí free ‚Üí OpenAI.

G2. Integra√ß√£o

/kb/ingest/link|file ‚Üí item entra em /curation/pending.

Aprovar item em /curation/action ‚Üí enfileira indexa√ß√£o ‚Üí snapshot salvo.

/api/chat e /api/chat/stream funcionam e degradam corretamente.

G3. E2E

Subir Embed API + Model API no Colab.

Backend Node com .env apontando para Colab URLs.

Ingerir um PDF ‚Üí aprovar ‚Üí perguntar no chat ‚Üí observar RAG.

Derrubar Colab ‚Üí ver fallback a GPU worker 2 ou provider free ‚Üí logs coerentes.

‚öôÔ∏è AP√äNDICE H ‚Äî CHECKLIST DE PRODU√á√ÉO

 Colar Ap√™ndice A (server) completo.

 Subir Ap√™ndice B (Python) ‚Äî Model/Embed APIs.

 Conectar frontend (ChatPage com SSE + pain√©is admin).

 Configurar GPU_WORKERS e URLs do Colab/Kaggle.

 Validar /health, /metrics, /api/chat, /api/chat/stream.

 Realizar Test Plan (G).

 Fixar limites de custo e regras de fallback no painel de Policy.