# AION - Sistema de IA Aut√¥nomo

## Overview
AION is an enterprise-grade autonomous AI system designed for robustness, flexibility, and self-operation, extending beyond current LLM limitations. It features configurable policy enforcement, RAG-based knowledge retrieval, advanced autonomous agents using POMDP with a ReAct framework, and professional video generation. The system provides a chat interface for end-users and an administrative dashboard with a 7-Trait Personality Equalizer. Operating in a single-tenant mode for optimized deployment and cost efficiency, AION's business vision is to deliver a self-sustaining, continuously evolving AI that learns and improves autonomously, reducing reliance on external APIs over time. It incorporates production-ready autonomous meta-learning capabilities, including a Meta-Learner Service, an Adaptive Mixture of Experts (ShiftEx MoE), a Personalized expert selection (PM-MoE Aggregator), and a Self-Improvement Engine for autonomous code analysis and patching.

## Recent Changes (November 2025)

### Privacy-Preserving Heuristics Implementation (COMPLETED)
- **90% Privacy Protection via Heuristics**: Production-ready privacy stack WITHOUT formal Differential Privacy
  - **Adaptive Thresholds**: Development (5 examples), Production (25 examples cohort protection), Sensitive (50 examples, federated=100)
  - **LoRA Fine-Tuning**: Reduces memorization 10,000x vs full fine-tuning (rank=16 prod, rank=8 sensitive)
  - **PII Redaction Service**: 10+ pattern types with false-positive filtering
    - Email (expanded TLDs), Phone (US/BR/UK formats), SSN/CPF/CNPJ, Credit Cards (Visa/MC/AmEx/Diners)
    - IP Addresses (RFC1918-compliant: skips 127.x, 10.x, 192.168.x, 172.16-31.x, 169.254.x)
    - Full Names (4-guard system: whitelist countries/cities, corporate indicators, sentence start detection, time words)
    - Sensitive URL params (token, api_key, password, etc)
  - **Replay Buffer**: Anti-catastrophic forgetting (PostgreSQL-backed, LRU eviction)
  - **Quality Gates**: Min score validation, length checks, HITL approval
- **Differential Privacy**: Disabled across all modes (requires custom GPU workers with Opacus/TF Privacy - outside current scope)
  - Config retained for future implementation but `enabled: false` with honest disclaimers
  - Removed misleading DP budget tracking that only logged metadata without real gradient clipping/noise
- **Honest Messaging**: System logs accurately describe heuristic-only privacy (threshold + LoRA + replay + PII)

### Meta-Learning Multimodal KB Expansion
- **Comprehensive KB Access**: Meta-Learning pipeline (`checkNewCuratedData` and `detectDataShifts`) now queries ALL 5 knowledge base sources:
  - `trainingDataCollection` (approved training examples)
  - `documents` (PDF, DOCX, XLSX, TXT, manual text)
  - `conversations` (chat messages and user interactions)
  - `curationQueue` (approved curated content with attachments)
  - `knowledgeSources` (web scraping, links, YouTube transcripts)
- **Data Source Breakdown**: Pipeline provides detailed count breakdown across all sources for transparency
- **Shift Detection Enhanced**: `detectDataShifts` analyzes recent data from ALL sources in parallel for comprehensive MMD calculation

### Namespace Isolation Fix (CRITICAL)
- **Schema Updates**: Added `namespace: text("namespace")` field to `conversations` and `knowledgeSources` tables with indexes for multi-tenant isolation
  - Migration executed via `npm run db:push` (Drizzle auto-migration)
  - Indexes created: `conversations_namespace_idx`, `knowledge_sources_namespace_idx`
  - Nullable field allows gradual adoption (NULL = global/unscoped data)
- **Backfill Strategy**: Existing records remain with `namespace = NULL` (treated as global data)
  - Future records can be tagged with namespace during creation
  - Optional: run manual backfill SQL to assign namespaces to historical data based on semantic analysis
  - No immediate action required - system works with mixed NULL/scoped data
- **Complete Filtering**: ALL 5 KB sources now respect namespace parameter:
  - `trainingDataCollection`: Direct namespace match
  - `documents`: JSONB array contains filter on `metadata.namespaces`
  - `conversations`: Direct namespace match (NEW field - `eq(conversations.namespace, namespace)`)
  - `curationQueue`: JSONB array contains filter on `suggestedNamespaces`
  - `knowledgeSources`: Direct namespace match (NEW field - `eq(knowledgeSources.namespace, namespace)`)
- **Cross-Tenant Protection**: Namespace-scoped pipeline runs now guarantee isolation between different tenants/domains
  - When `namespace` parameter provided: filters ALL 5 sources
  - When `namespace` omitted: queries ALL data (global mode)
- **API/Storage Contract Changes**:
  - Meta-Learning Orchestrator: `checkNewCuratedData(namespace?)` and `detectDataShifts(namespace?)` now filter all sources
  - Storage layer: No changes required (schema-only update)
  - Frontend: No changes required (backend-only filtering)
- **TypeScript Clean**: Fixed circular reference errors in `moeExperts` schema (self-referencing foreign keys)

### Admin Interface Consolidation
- **Hierarchical Menu**: Reduced from 21 flat navigation items to 8 logical categories with visual grouping:
  1. **Command Center**: Overview dashboard
  2. **Analytics & Monitoring**: Telemetry, Token Monitoring, History, Cost History
  3. **Knowledge & Vision**: KB, Images, Image Search, Vision System
  4. **Autonomous AI**: Meta-Learning (includes Auto-Evolution metrics)
  5. **Training Fabric**: Datasets, Jobs, Federated Training
  6. **Operations**: GPU Management, Curation Queue
  7. **Access Governance**: Agents, Users, Permissions
  8. **System Configuration**: Namespaces, Lifecycle, Settings
- **Auto-Evolution Integration**: Standalone Auto-Evolution tab removed, metrics consolidated into Meta-Learning dashboard to reduce redundancy
- **Full i18n Coverage**: All 8 categories translated across PT-BR (default), EN-US, ES-ES
- **All Features Accessible**: Complete preservation of original 21 features through hierarchical organization

## User Preferences
Estilo de comunica√ß√£o preferido: Linguagem simples e cotidiana.

**REGRA FUNDAMENTAL DE TRABALHO:**
1. **SEMPRE responda d√∫vidas do usu√°rio primeiro**
2. **SEMPRE continue tarefas em andamento at√© o final**
3. **NUNCA deixe tarefas incompletas para tr√°s**
4. Se o usu√°rio pedir novas atividades ‚Üí adicione √† fila AP√ìS as tarefas atuais
5. Fluxo obrigat√≥rio: Responder ‚Üí Completar tarefas atuais ‚Üí Iniciar novas tarefas
6. **NUNCA comece tarefas novas antes de terminar as antigas**

**üö® REGRA CR√çTICA DE QUALIDADE - ZERO TOLER√ÇNCIA:**
**"NADA NIVEL MVP - JA NASCE NIVEL PRODU√á√ÉO"**
- ‚ùå **ZERO strings hardcoded** - TUDO deve usar i18n (PT/EN/ES)
- ‚ùå **ZERO dados mocados** - TUDO deve vir do PostgreSQL
- ‚ùå **ZERO in-memory storage** - TUDO deve ser persistente no DB
- ‚ùå **ZERO implementa√ß√µes incompletas** - TUDO deve ser funcional e production-ready
- ‚úÖ **100% dados reais e configur√°veis** - Todas as features devem ser totalmente funcionais
- ‚úÖ **100% persist√™ncia** - Todos os dados devem ser salvos no banco de dados
- ‚úÖ **100% internacionalizado** - Todas as strings devem estar traduzidas em 3 idiomas
- ‚úÖ **100% production-grade** - C√≥digo, valida√ß√µes, error handling completos

**IMPLEMENTA√á√ÉO OBRIGAT√ìRIA:**
- Sempre verificar se strings est√£o traduzidas antes de entregar
- Sempre usar PostgreSQL via Drizzle ORM, nunca in-memory
- Sempre implementar valida√ß√µes, error handling, loading states
- Sempre adicionar data-testid para testes E2E
- Sempre revisar c√≥digo com architect antes de marcar como completed

## System Architecture

### Core System Design
AION operates in a single-tenant mode with a multi-agent architecture and LLM-driven Mixture of Experts (MoE) routing based on intent classification. It includes a 5-level priority chain automatic fallback system and universal multi-language support. Specialized agents with dedicated knowledge base namespaces, tool access, and budget limits are supported. A Human-in-the-Loop (HITL) knowledge curation system, backed by PostgreSQL, requires human approval for all content before indexing with a Zero Bypass Policy. A production-ready GPU Pool System manages worker detection, heartbeat monitoring, load balancing, inference priority, and schedule-based rotation. A Continuous Self-Evolution System collects high-quality conversations for instruction tuning and dataset generation, applying HITL. Full Multimodal Processing supports various document types, images, videos, YouTube transcripts, and Deep Web Crawling. The Vision Cascade System provides automatic failover across 5 providers with quota tracking. The agent system includes a level-based hierarchy for agents and sub-agents with cascading deletion. Federated Learning is fully implemented with a Gradient Aggregation Coordinator and Fault Tolerance. User & RBAC Management provides enterprise-level user and permission management with a granular permission system. The system incorporates a Meta-Learner Service, Adaptive Mixture of Experts (ShiftEx MoE) with MMD shift detection and dynamic expert spawning, a Personalized Expert Selection (PM-MoE Aggregator), and a Self-Improvement Engine for autonomous code analysis.

### UI/UX
The frontend is built with React 18, Vite, Wouter, and TanStack Query, utilizing Radix UI, shadcn/ui patterns, Tailwind CSS, and a custom HSL-based design system. It features an elegant minimalist design with modern glassmorphism. It offers a clean conversational chat interface and an enterprise-level Admin Panel with side navigation and full Internationalization (i18n) supporting PT-BR (default), EN-US, ES-ES. All administrative pages are translated, including Dataset, Agent, and Curation Queue management. The Curation Queue for HITL review supports filtering and batch actions. Vision System monitoring displays real-time quota tracking across 5 providers. AI-powered semantic image search and Health Diagnostics are included. The Personality Equalizer in the Settings tab offers granular control via 7 functional sliders. The Admin Panel design philosophy balances auto-creation with manual management for auditing, correction, hygiene, seeding, override, and monitoring for namespaces, and customization, specialized configuration, hierarchy management, testing, migration, and emergency control for agents.

### Technical Implementations
The backend uses Node.js and TypeScript with Express.js and PostgreSQL via Drizzle ORM. Key services include LLM Client, Storage, Multi-Agent Router (MoE), namespace-scoped RAG, Agent Engine (ReAct with POMDP), Automatic Fallback, Production-Grade Multimodal Processing, Web Content Discovery, YouTube Transcription Service, Vision Cascade, free LLM Provider rotation, GPU Orchestrator, GPU Pool Manager, GPU Load Balancer, Training Data Collector, Dataset Generator, Auto-Learning System, Token Monitoring, Lifecycle Management, Orphan Detection, Validation (Zod schemas), and a Comprehensive Telemetry System. It includes a Kaggle CLI Service for automatic provisioning and credential bootstrapping, a Colab Orchestrator Service for Puppeteer automation, and a GPU Management UI. A GPU Deletion Service handles cascading deletes, and an Auto-Scaling Service with a 24/7 Orchestrator intelligently manages all GPUs (Colab + Kaggle) using multi-factor dispatching and staggered rotation strategies. The Namespace Classifier uses LLM-based auto-classification. A Persistent Vector Store is PostgreSQL-backed with LRU cache. Robust error handling and structured logging are implemented. The frontend implements a centralized i18n system and uses Replit Auth (OpenID Connect). RAG combines OpenAI embeddings with BM25. Professional video generation uses an asynchronous job queue. Multi-Cloud Deployment uses Google Cloud Run and AWS Fargate. Training data validation and a Lifecycle Management System are in place.

### System Design Decisions
Key decisions include single-tenant architecture and externalized JSON behavioral configurations for dynamic updates. Comprehensive observability and telemetry include query monitoring, granular hierarchical usage analytics, a modern dashboard with Recharts visualizations, PostgreSQL trigram indexes for optimized search performance, and 29 production-ready REST endpoints for metric access.

## External Dependencies

### Third-Party Services
- **API OpenAI**: LLM completions, embeddings, function calling, GPT-4o Vision.
- **Neon Database**: PostgreSQL Serverless.
- **Google Cloud Run**: Primary deployment platform.
- **AWS Fargate**: Backup deployment platform.
- **DuckDuckGo**: Web search.
- **OpenRouter**: Free LLM API gateway (GPT-4V, Claude 3 Haiku).
- **Groq, Gemini, HuggingFace**: Free LLM API providers.
- **Google Colab, Kaggle, Modal**: Free GPU resources for fine-tuning and inference.
- **RunPod/Modal**: GPU workers for video generation.
- **Replit**: Development environment and authentication (OpenID Connect).

### Core Libraries (NPM)
- **@neondatabase/serverless**: PostgreSQL client
- **drizzle-orm**: Type-safe ORM
- **openai**: Official OpenAI SDK
- **@google/generative-ai**: Gemini API client
- **@huggingface/inference**: HuggingFace API client
- **groq-sdk**: Groq API client
- **youtube-transcript**: YouTube caption/subtitle extraction
- **@radix-ui/**: Accessible UI primitives
- **@tanstack/react-query**: Server state management
- **tailwindcss**: Utility-first CSS framework
- **zod**: Schema validation
- **mammoth**: DOCX to text extraction
- **xlsx**: Excel file parsing
- **xml2js**: XML parsing
- **pdf-parse**: PDF text extraction
- **cheerio**: HTML parsing and web scraping
- **multer**: File upload handling
- **file-type**: MIME type detection