{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AION - LoRA Fine-Tuning Worker (Google Colab)\n",
        "\n",
        "Este notebook:\n",
        "1. Conecta ao servidor AION principal\n",
        "2. Baixa datasets automaticamente\n",
        "3. Treina modelos com LoRA (Parameter-Efficient Fine-Tuning)\n",
        "4. Serve o modelo via API (Ngrok)\n",
        "5. Envia heartbeat para o AION saber que est√° online\n",
        "\n",
        "## Configura√ß√£o Inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vari√°veis de configura√ß√£o\n",
        "AION_SERVER_URL = \"https://your-replit-app.replit.dev\"  # ALTERE AQUI\n",
        "WORKER_ID = 1  # ID √∫nico deste worker\n",
        "PROVIDER = \"colab\"  # colab, kaggle, ou modal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Instala√ß√£o de Depend√™ncias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.36.2\n",
        "!pip install -q peft==0.7.1\n",
        "!pip install -q datasets==2.16.1\n",
        "!pip install -q accelerate==0.25.0\n",
        "!pip install -q bitsandbytes==0.41.3\n",
        "!pip install -q scipy\n",
        "!pip install -q flask\n",
        "!pip install -q pyngrok\n",
        "!pip install -q requests\n",
        "\n",
        "print(\"‚úÖ Depend√™ncias instaladas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configurar Ngrok (Para expor API publicamente)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# OBTENHA SEU TOKEN GR√ÅTIS EM: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "NGROK_TOKEN = \"\"  # COLE SEU TOKEN AQUI\n",
        "\n",
        "if NGROK_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "    print(\"‚úÖ Ngrok configurado!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Configure NGROK_TOKEN para conectar ao AION\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Fun√ß√£o de Download de Dataset do AION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def download_dataset(dataset_url, output_path=\"/content/dataset.jsonl\"):\n",
        "    \"\"\"\n",
        "    Baixa dataset do servidor AION\n",
        "    \"\"\"\n",
        "    print(f\"üì• Baixando dataset de {dataset_url}...\")\n",
        "    \n",
        "    response = requests.get(dataset_url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    with open(output_path, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "    \n",
        "    # Contar exemplos\n",
        "    with open(output_path, 'r') as f:\n",
        "        num_examples = sum(1 for line in f)\n",
        "    \n",
        "    print(f\"‚úÖ Dataset baixado: {num_examples} exemplos em {output_path}\")\n",
        "    return output_path, num_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Fun√ß√£o de Fine-Tuning com LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "\n",
        "def train_lora_model(\n",
        "    dataset_path,\n",
        "    base_model=\"meta-llama/Llama-2-7b-chat-hf\",  # Ou \"mistralai/Mistral-7B-v0.1\"\n",
        "    lora_r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    epochs=3,\n",
        "    batch_size=4,\n",
        "    learning_rate=2e-4,\n",
        "    output_dir=\"/content/lora_model\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Treina modelo com LoRA\n",
        "    \"\"\"\n",
        "    print(f\"\\nüèãÔ∏è Iniciando fine-tuning LoRA...\")\n",
        "    print(f\"   Base model: {base_model}\")\n",
        "    print(f\"   LoRA r={lora_r}, alpha={lora_alpha}\")\n",
        "    print(f\"   Epochs: {epochs}, Batch: {batch_size}, LR: {learning_rate}\")\n",
        "    \n",
        "    # 1. Carregar tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # 2. Carregar modelo base (quantizado para 4-bit para economizar VRAM)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        load_in_4bit=True,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    # 3. Preparar modelo para LoRA\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    \n",
        "    # 4. Configurar LoRA\n",
        "    lora_config = LoraConfig(\n",
        "        r=lora_r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],  # Apenas attention matrices\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    \n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "    \n",
        "    # 5. Carregar dataset\n",
        "    dataset = load_dataset('json', data_files=dataset_path, split='train')\n",
        "    \n",
        "    # 6. Preprocessar\n",
        "    def preprocess(examples):\n",
        "        # Formato Alpaca: instruction + input + output\n",
        "        texts = []\n",
        "        for i in range(len(examples['instruction'])):\n",
        "            instruction = examples['instruction'][i]\n",
        "            output = examples['output'][i]\n",
        "            text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
        "            texts.append(text)\n",
        "        \n",
        "        return tokenizer(texts, truncation=True, max_length=512, padding=\"max_length\")\n",
        "    \n",
        "    tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=dataset.column_names)\n",
        "    \n",
        "    # 7. Configurar treinamento\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=learning_rate,\n",
        "        fp16=True,\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=10,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "    \n",
        "    # 8. Treinar!\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "    )\n",
        "    \n",
        "    print(\"\\nüöÄ Iniciando treinamento...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    # 9. Salvar modelo\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Modelo treinado e salvo em {output_dir}\")\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Servidor API para Infer√™ncia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from threading import Thread\n",
        "import time\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Vari√°veis globais\n",
        "model = None\n",
        "tokenizer = None\n",
        "training_status = {\"status\": \"idle\", \"progress\": 0}\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\n",
        "        \"status\": \"online\",\n",
        "        \"worker_id\": WORKER_ID,\n",
        "        \"provider\": PROVIDER,\n",
        "        \"model_loaded\": model is not None,\n",
        "        \"training\": training_status\n",
        "    })\n",
        "\n",
        "@app.route('/v1/chat/completions', methods=['POST'])\n",
        "def chat_completions():\n",
        "    \"\"\"Infer√™ncia compat√≠vel com OpenAI API\"\"\"\n",
        "    if model is None or tokenizer is None:\n",
        "        return jsonify({\"error\": \"Model not loaded\"}), 503\n",
        "    \n",
        "    data = request.json\n",
        "    messages = data.get('messages', [])\n",
        "    max_tokens = data.get('max_tokens', 512)\n",
        "    temperature = data.get('temperature', 0.7)\n",
        "    \n",
        "    # Converter mensagens para prompt\n",
        "    prompt = \"\\n\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
        "    prompt += \"\\n\\nassistant:\"\n",
        "    \n",
        "    # Gerar resposta\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    \n",
        "    return jsonify({\n",
        "        \"choices\": [{\n",
        "            \"message\": {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": response.strip()\n",
        "            },\n",
        "            \"finish_reason\": \"stop\"\n",
        "        }],\n",
        "        \"usage\": {\n",
        "            \"prompt_tokens\": inputs['input_ids'].shape[1],\n",
        "            \"completion_tokens\": outputs.shape[1] - inputs['input_ids'].shape[1],\n",
        "            \"total_tokens\": outputs.shape[1]\n",
        "        }\n",
        "    })\n",
        "\n",
        "@app.route('/train', methods=['POST'])\n",
        "def train_endpoint():\n",
        "    \"\"\"Recebe job de treino do AION\"\"\"\n",
        "    global training_status\n",
        "    \n",
        "    data = request.json\n",
        "    job_id = data.get('jobId')\n",
        "    dataset_url = data.get('dataset')\n",
        "    config = data.get('lora', {})\n",
        "    \n",
        "    # Iniciar treino em background\n",
        "    def train_async():\n",
        "        global model, tokenizer, training_status\n",
        "        try:\n",
        "            training_status = {\"status\": \"downloading\", \"progress\": 10}\n",
        "            dataset_path, _ = download_dataset(dataset_url)\n",
        "            \n",
        "            training_status = {\"status\": \"training\", \"progress\": 30}\n",
        "            model, tokenizer = train_lora_model(\n",
        "                dataset_path,\n",
        "                lora_r=config.get('r', 8),\n",
        "                lora_alpha=config.get('alpha', 16),\n",
        "                lora_dropout=config.get('dropout', 0.05)\n",
        "            )\n",
        "            \n",
        "            training_status = {\"status\": \"completed\", \"progress\": 100}\n",
        "            \n",
        "            # Notificar AION\n",
        "            requests.post(f\"{AION_SERVER_URL}/api/gpu/training-complete\", json={\n",
        "                \"workerId\": WORKER_ID,\n",
        "                \"jobId\": job_id,\n",
        "                \"status\": \"completed\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            training_status = {\"status\": \"failed\", \"error\": str(e)}\n",
        "    \n",
        "    Thread(target=train_async, daemon=True).start()\n",
        "    \n",
        "    return jsonify({\"status\": \"training_started\", \"jobId\": job_id})\n",
        "\n",
        "def run_server():\n",
        "    app.run(host='0.0.0.0', port=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Heartbeat para AION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def send_heartbeat(ngrok_url):\n",
        "    \"\"\"\n",
        "    Envia heartbeat para AION saber que worker est√° online\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            requests.post(f\"{AION_SERVER_URL}/api/gpu/heartbeat\", json={\n",
        "                \"workerId\": WORKER_ID,\n",
        "                \"provider\": PROVIDER,\n",
        "                \"ngrokUrl\": ngrok_url,\n",
        "                \"status\": \"online\",\n",
        "                \"capabilities\": {\n",
        "                    \"model\": \"llama-2-7b-lora\",\n",
        "                    \"gpu\": \"T4\",\n",
        "                    \"vram_gb\": 16,\n",
        "                    \"max_concurrent\": 1\n",
        "                }\n",
        "            })\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        time.sleep(30)  # A cada 30 segundos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. INICIAR WORKER üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iniciar servidor Flask em background\n",
        "server_thread = Thread(target=run_server, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "time.sleep(3)  # Aguardar servidor iniciar\n",
        "\n",
        "# Abrir t√∫nel Ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"\\nüåê API p√∫blica: {public_url}\")\n",
        "print(f\"üîó Health check: {public_url}/health\")\n",
        "\n",
        "# Registrar no AION\n",
        "response = requests.post(f\"{AION_SERVER_URL}/api/gpu/register\", json={\n",
        "    \"workerId\": WORKER_ID,\n",
        "    \"provider\": PROVIDER,\n",
        "    \"ngrokUrl\": str(public_url),\n",
        "    \"capabilities\": {\n",
        "        \"model\": \"llama-2-7b-lora\",\n",
        "        \"gpu\": \"T4\",\n",
        "        \"vram_gb\": 16\n",
        "    }\n",
        "})\n",
        "\n",
        "print(f\"\\n‚úÖ Worker registrado no AION!\")\n",
        "print(f\"Response: {response.json()}\")\n",
        "\n",
        "# Iniciar heartbeat\n",
        "heartbeat_thread = Thread(target=send_heartbeat, args=(str(public_url),), daemon=True)\n",
        "heartbeat_thread.start()\n",
        "\n",
        "print(\"\\nüíö WORKER ATIVO - Aguardando jobs de treino...\")\n",
        "print(\"Copie a URL do Ngrok e use no AION Admin Dashboard\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Keep-Alive (Manter Colab Ativo)\n",
        "\n",
        "**Abra o Console do Browser (Ctrl+Shift+I) e execute:**\n",
        "\n",
        "```javascript\n",
        "function ClickConnect() {\n",
        "  console.log('AION Keep-Alive Active');\n",
        "  document.querySelector(\"colab-connect-button\")?.shadowRoot.querySelector(\"#connect\")?.click();\n",
        "}\n",
        "setInterval(ClickConnect, 60000);\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manter c√©lula executando para evitar idle timeout\n",
        "while True:\n",
        "    time.sleep(600)  # 10 minutos\n",
        "    print(f\"‚è∞ Worker ainda ativo - {time.strftime('%H:%M:%S')}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
