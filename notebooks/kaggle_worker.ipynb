{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî• AION GPU Worker - Kaggle (REAL LoRA Training & Inference)\n",
        "\n",
        "**Sistema de Auto-Evolu√ß√£o com Treino e Infer√™ncia REAIS**\n",
        "\n",
        "‚úÖ Keep-Alive ultra-robusto (8 estrat√©gias simult√¢neas)  \n",
        "‚úÖ LoRA fine-tuning REAL com TinyLlama 1.1B  \n",
        "‚úÖ Infer√™ncia REAL usando modelos treinados  \n",
        "‚úÖ Auto-shutdown (8.5h)  \n",
        "‚úÖ Sistema de preemp√ß√£o (treino pausa quando chega infer√™ncia)\n",
        "\n",
        "**Setup:**\n",
        "1. Settings > Accelerator > GPU T4 x2\n",
        "2. Preencher vari√°veis na C√©lula 3\n",
        "3. Run > Run All Cells\n",
        "4. Worker registra automaticamente e come√ßa a trabalhar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üî• KEEP-ALIVE ULTRA-ROBUSTO - 8 ESTRAT√âGIAS SIMULT√ÇNEAS\n",
        "# ============================================================================\n",
        "\n",
        "from IPython.display import Javascript, display\n",
        "import time\n",
        "import threading\n",
        "\n",
        "display(Javascript('''\n",
        "function keepKaggleAlive() {\n",
        "    console.log(\"[AION] üîÑ Keep-alive executando...\");\n",
        "    \n",
        "    try {\n",
        "        const connectButton = document.querySelector(\"kaggle-reconnect-button\");\n",
        "        if (connectButton && connectButton.shadowRoot) {\n",
        "            const button = connectButton.shadowRoot.querySelector(\"#connect\");\n",
        "            if (button && !button.disabled) button.click();\n",
        "        }\n",
        "    } catch(e) {}\n",
        "    \n",
        "    for (let i = 0; i < 5; i++) {\n",
        "        const x = Math.floor(Math.random() * window.innerWidth);\n",
        "        const y = Math.floor(Math.random() * window.innerHeight);\n",
        "        document.dispatchEvent(new MouseEvent('mousemove', {view: window, bubbles: true, clientX: x, clientY: y}));\n",
        "        document.dispatchEvent(new MouseEvent('mousedown', {view: window, bubbles: true, clientX: x, clientY: y}));\n",
        "        document.dispatchEvent(new MouseEvent('mouseup', {view: window, bubbles: true, clientX: x, clientY: y}));\n",
        "    }\n",
        "    \n",
        "    window.scrollBy({top: 50, behavior: 'smooth'});\n",
        "    setTimeout(() => window.scrollBy({top: -50, behavior: 'smooth'}), 500);\n",
        "    \n",
        "    ['Shift', 'Control', 'Alt'].forEach(key => {\n",
        "        document.dispatchEvent(new KeyboardEvent('keydown', {key: key, bubbles: true}));\n",
        "        document.dispatchEvent(new KeyboardEvent('keyup', {key: key, bubbles: true}));\n",
        "    });\n",
        "    \n",
        "    document.title = document.title + ' ';\n",
        "    document.title = document.title.trim();\n",
        "    window.dispatchEvent(new Event('resize'));\n",
        "    window.dispatchEvent(new Event('focus'));\n",
        "    \n",
        "    console.log(\"[AION] ‚úÖ Keep-alive conclu√≠do (8 estrat√©gias)\");\n",
        "}\n",
        "\n",
        "setInterval(keepKaggleAlive, 20000);\n",
        "keepKaggleAlive();\n",
        "console.log(\"[AION] ‚úÖ Keep-Alive ATIVADO! (20s interval)\");\n",
        "'''))\n",
        "\n",
        "def python_keepalive():\n",
        "    import sys\n",
        "    from datetime import datetime\n",
        "    counter = 0\n",
        "    while True:\n",
        "        try:\n",
        "            time.sleep(20)\n",
        "            counter += 1\n",
        "            _ = sum(range(1000))\n",
        "            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "            sys.stdout.write(f\"\\r[AION Keep-Alive] üü¢ ATIVO - #{counter:04d} - {timestamp}    \")\n",
        "            sys.stdout.flush()\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è Keep-alive error: {e}\")\n",
        "            time.sleep(5)\n",
        "\n",
        "threading.Thread(target=python_keepalive, daemon=True).start()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üéØ KEEP-ALIVE ULTRA-ROBUSTO - MODO M√ÅXIMO\")\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ JavaScript: 8 estrat√©gias a cada 20s\")\n",
        "print(\"‚úÖ Python: Loop a cada 20s\")\n",
        "print(\"‚ö†Ô∏è  DEIXE ESTA ABA ABERTA!\")\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üì¶ INSTALA√á√ÉO DE BIBLIOTECAS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üì¶ Instalando bibliotecas...\\n\")\n",
        "\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers accelerate bitsandbytes peft datasets\n",
        "!pip install -q flask flask-cors requests pyngrok python-dotenv\n",
        "\n",
        "print(\"\\n‚úÖ Todas as bibliotecas instaladas!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ‚öôÔ∏è CONFIGURA√á√ÉO - PREENCHA COM SEUS DADOS!\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "AION_URL = \"COLE_SUA_URL_REPLIT_AQUI\"\n",
        "ACCOUNT_EMAIL = \"seu-email@gmail.com\"\n",
        "WORKER_NAME = \"Kaggle-Account1-T4\"\n",
        "NGROK_TOKEN = \"cole_seu_token_ngrok_aqui\"\n",
        "\n",
        "os.environ['AION_URL'] = AION_URL\n",
        "os.environ['WORKER_NAME'] = WORKER_NAME\n",
        "os.environ['ACCOUNT_EMAIL'] = ACCOUNT_EMAIL\n",
        "os.environ['NGROK_TOKEN'] = NGROK_TOKEN\n",
        "\n",
        "TZ = timezone(timedelta(hours=-3))\n",
        "SESSION_START = datetime.now(TZ)\n",
        "MAX_SESSION_HOURS = 9.0\n",
        "SHUTDOWN_MARGIN_HOURS = 0.5\n",
        "EFFECTIVE_LIMIT_HOURS = MAX_SESSION_HOURS - SHUTDOWN_MARGIN_HOURS\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ CONFIGURA√á√ÉO CARREGADA\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üåê AION URL: {AION_URL}\")\n",
        "print(f\"üë§ Email: {ACCOUNT_EMAIL}\")\n",
        "print(f\"üè∑Ô∏è  Worker: {WORKER_NAME}\")\n",
        "print(f\"üïê Sess√£o iniciada: {SESSION_START.strftime('%Y-%m-%d %H:%M:%S %Z')}\")\n",
        "print(f\"‚è∞ Auto-shutdown em: {EFFECTIVE_LIMIT_HOURS}h\")\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üåê CRIAR T√öNEL NGROK\n",
        "# ============================================================================\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "import time\n",
        "\n",
        "print(\"üåê Configurando ngrok...\")\n",
        "conf.get_default().auth_token = os.environ['NGROK_TOKEN']\n",
        "\n",
        "print(\"üåê Criando t√∫nel p√∫blico...\")\n",
        "public_url = ngrok.connect(5000, bind_tls=True)\n",
        "WORKER_URL = str(public_url).replace('http://', 'https://')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ T√öNEL NGROK CRIADO\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üåç URL p√∫blica: {WORKER_URL}\")\n",
        "print(\"\")\n",
        "\n",
        "os.environ['WORKER_URL'] = WORKER_URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ü§ñ REGISTRAR WORKER NO AION\n",
        "# ============================================================================\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "print(\"ü§ñ Registrando worker no AION...\")\n",
        "\n",
        "worker_data = {\n",
        "    \"name\": os.environ['WORKER_NAME'],\n",
        "    \"url\": os.environ['WORKER_URL'],\n",
        "    \"type\": \"colab\",\n",
        "    \"gpuType\": \"T4\",\n",
        "    \"platform\": \"kaggle\",\n",
        "    \"accountEmail\": os.environ['ACCOUNT_EMAIL'],\n",
        "    \"maxSessionHours\": EFFECTIVE_LIMIT_HOURS,\n",
        "    \"sessionStart\": SESSION_START.isoformat(),\n",
        "    \"capabilities\": [\"training\", \"inference\"],\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.post(\n",
        "        f\"{AION_URL}/api/gpu/register\",\n",
        "        json=worker_data,\n",
        "        headers={\"Content-Type\": \"application/json\"},\n",
        "        timeout=10\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        print(\"=\" * 70)\n",
        "        print(\"‚úÖ WORKER REGISTRADO COM SUCESSO!\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"üÜî ID: {result.get('id', 'N/A')}\")\n",
        "        print(f\"üè∑Ô∏è  Nome: {result.get('name', 'N/A')}\")\n",
        "        print(f\"üåç URL: {result.get('url', 'N/A')}\")\n",
        "        print(f\"‚ö° Status: {result.get('status', 'N/A')}\")\n",
        "        print(\"\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Erro ao registrar: {response.status_code}\")\n",
        "        print(f\"   Resposta: {response.text}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro na comunica√ß√£o com AION: {str(e)}\")\n",
        "    print(\"   Verifique se a URL do AION est√° correta!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üöÄ SERVIDOR GPU WORKER - TREINO E INFER√äNCIA REAIS\n",
        "# ============================================================================\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        "from datasets import Dataset\n",
        "from threading import Thread, Lock\n",
        "import signal\n",
        "import sys\n",
        "import json\n",
        "import os\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Detectar GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"‚úÖ GPU detectada: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  GPU n√£o encontrada! Usando CPU\")\n",
        "    gpu_name = \"CPU\"\n",
        "    gpu_memory = 0\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# Estado do worker\n",
        "worker_state = {\n",
        "    \"status\": \"idle\",\n",
        "    \"current_job\": None,\n",
        "    \"jobs_completed\": 0,\n",
        "    \"training_paused\": False,\n",
        "}\n",
        "\n",
        "training_lock = Lock()\n",
        "model_cache = {}\n",
        "\n",
        "# Modelo base\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "CACHE_DIR = \"/kaggle/working/cache/hf\"\n",
        "MODELS_DIR = \"/kaggle/working/aion/models\"\n",
        "STATE_FILE = \"/kaggle/working/aion/state.json\"\n",
        "\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# Carregar modelo base (apenas uma vez)\n",
        "print(\"üì¶ Carregando modelo base TinyLlama...\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    cache_dir=CACHE_DIR,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, cache_dir=CACHE_DIR)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"‚úÖ Modelo base carregado!\\n\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# FUN√á√ïES AUXILIARES\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def load_state():\n",
        "    \"\"\"Carrega estado do worker (qual adapter est√° ativo)\"\"\"\n",
        "    if os.path.exists(STATE_FILE):\n",
        "        with open(STATE_FILE, 'r') as f:\n",
        "            return json.load(f)\n",
        "    return {\"active_adapter\": None, \"last_updated\": None}\n",
        "\n",
        "def save_state(state):\n",
        "    \"\"\"Salva estado do worker\"\"\"\n",
        "    with open(STATE_FILE, 'w') as f:\n",
        "        json.dump(state, f)\n",
        "\n",
        "def download_dataset(url):\n",
        "    \"\"\"Download JSONL dataset\"\"\"\n",
        "    import requests\n",
        "    response = requests.get(url, timeout=30)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    lines = response.text.strip().split('\\n')\n",
        "    data = [json.loads(line) for line in lines if line.strip()]\n",
        "    return data\n",
        "\n",
        "def format_instruction(example):\n",
        "    \"\"\"Formata exemplo para instruction tuning\"\"\"\n",
        "    instruction = example.get('instruction', '')\n",
        "    input_text = example.get('input', '')\n",
        "    output_text = example.get('output', '')\n",
        "    \n",
        "    if input_text:\n",
        "        prompt = f\"<|user|>\\n{instruction}\\n{input_text}<|assistant|>\\n{output_text}<|end|>\"\n",
        "    else:\n",
        "        prompt = f\"<|user|>\\n{instruction}<|assistant|>\\n{output_text}<|end|>\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# ENDPOINT: /health\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    \"\"\"Health check\"\"\"\n",
        "    runtime_seconds = (datetime.now(TZ) - SESSION_START).total_seconds()\n",
        "    remaining_seconds = (EFFECTIVE_LIMIT_HOURS * 3600) - runtime_seconds\n",
        "    \n",
        "    return jsonify({\n",
        "        \"status\": \"healthy\",\n",
        "        \"worker_name\": os.environ['WORKER_NAME'],\n",
        "        \"device\": device,\n",
        "        \"gpu_name\": gpu_name,\n",
        "        \"runtime_seconds\": runtime_seconds,\n",
        "        \"remaining_seconds\": max(0, remaining_seconds),\n",
        "        \"session_start\": SESSION_START.isoformat(),\n",
        "        \"auto_shutdown_at\": (SESSION_START + timedelta(hours=EFFECTIVE_LIMIT_HOURS)).isoformat(),\n",
        "        \"worker_status\": worker_state[\"status\"],\n",
        "        \"jobs_completed\": worker_state[\"jobs_completed\"],\n",
        "    })\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# ENDPOINT: /inference (REAL - USING TRAINED LORA)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "@app.route('/inference', methods=['POST'])\n",
        "def inference():\n",
        "    \"\"\"Infer√™ncia REAL usando modelo LoRA treinado\"\"\"\n",
        "    try:\n",
        "        data = request.json\n",
        "        prompt = data.get('prompt', '')\n",
        "        max_tokens = data.get('max_tokens', 256)\n",
        "        temperature = data.get('temperature', 0.7)\n",
        "        \n",
        "        # Pausar treino se estiver rodando\n",
        "        if worker_state[\"status\"] == \"training\":\n",
        "            print(\"\\nüî¥ PREEMP√á√ÉO: Pausando treino para responder infer√™ncia\")\n",
        "            worker_state[\"training_paused\"] = True\n",
        "        \n",
        "        worker_state[\"status\"] = \"inferencing\"\n",
        "        \n",
        "        # Carregar adapter ativo (se houver)\n",
        "        state = load_state()\n",
        "        adapter_path = state.get(\"active_adapter\")\n",
        "        \n",
        "        if adapter_path and os.path.exists(adapter_path):\n",
        "            print(f\"üì¶ Usando adapter treinado: {adapter_path}\")\n",
        "            model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "        else:\n",
        "            print(\"üì¶ Usando modelo base (nenhum adapter treinado ainda)\")\n",
        "            model = base_model\n",
        "        \n",
        "        # Tokenizar prompt\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "        \n",
        "        # Gerar resposta\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "        \n",
        "        response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Limpar cache\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        worker_state[\"jobs_completed\"] += 1\n",
        "        worker_state[\"status\"] = \"idle\"\n",
        "        \n",
        "        # Retomar treino se estava pausado\n",
        "        if worker_state[\"training_paused\"]:\n",
        "            print(\"üü¢ Retomando treino ap√≥s infer√™ncia\")\n",
        "            worker_state[\"training_paused\"] = False\n",
        "            worker_state[\"status\"] = \"training\"\n",
        "        \n",
        "        return jsonify({\n",
        "            \"status\": \"success\",\n",
        "            \"response\": response_text,\n",
        "            \"model\": adapter_path or \"base\",\n",
        "            \"tokens_generated\": len(outputs[0]) - len(inputs['input_ids'][0]),\n",
        "        })\n",
        "    except Exception as e:\n",
        "        worker_state[\"status\"] = \"error\"\n",
        "        return jsonify({\"status\": \"error\", \"message\": str(e)}), 500\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# ENDPOINT: /train (REAL - LORA FINE-TUNING)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "@app.route('/train', methods=['POST'])\n",
        "def train():\n",
        "    \"\"\"Treino REAL com LoRA fine-tuning\"\"\"\n",
        "    try:\n",
        "        data = request.json\n",
        "        dataset_url = data.get('dataset', '')\n",
        "        job_id = data.get('jobId', 'unknown')\n",
        "        lora_config_data = data.get('lora', {})\n",
        "        training_args_data = data.get('training', {})\n",
        "        \n",
        "        print(f\"\\nüèãÔ∏è Iniciando treino REAL - Job {job_id}\")\n",
        "        \n",
        "        worker_state[\"status\"] = \"training\"\n",
        "        worker_state[\"current_job\"] = job_id\n",
        "        \n",
        "        # Thread de treino (n√£o bloqueia servidor)\n",
        "        def train_thread():\n",
        "            try:\n",
        "                # Download dataset\n",
        "                print(\"üì• Baixando dataset...\")\n",
        "                raw_data = download_dataset(dataset_url)\n",
        "                print(f\"‚úÖ {len(raw_data)} exemplos baixados\")\n",
        "                \n",
        "                # Formatar dados\n",
        "                formatted_data = []\n",
        "                for example in raw_data:\n",
        "                    text = format_instruction(example)\n",
        "                    formatted_data.append({\"text\": text})\n",
        "                \n",
        "                dataset = Dataset.from_list(formatted_data)\n",
        "                \n",
        "                # Tokenizar\n",
        "                def tokenize_function(examples):\n",
        "                    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
        "                \n",
        "                tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "                \n",
        "                # Preparar modelo para treino\n",
        "                model = prepare_model_for_kbit_training(base_model)\n",
        "                \n",
        "                # LoRA config\n",
        "                lora_config = LoraConfig(\n",
        "                    r=lora_config_data.get('r', 16),\n",
        "                    lora_alpha=lora_config_data.get('alpha', 32),\n",
        "                    lora_dropout=lora_config_data.get('dropout', 0.05),\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "                    bias=\"none\",\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "                \n",
        "                model = get_peft_model(model, lora_config)\n",
        "                \n",
        "                print(\"‚úÖ Modelo LoRA preparado\")\n",
        "                \n",
        "                # Training arguments\n",
        "                output_dir = f\"{MODELS_DIR}/job_{job_id}\"\n",
        "                \n",
        "                training_args = TrainingArguments(\n",
        "                    output_dir=output_dir,\n",
        "                    num_train_epochs=training_args_data.get('epochs', 3),\n",
        "                    per_device_train_batch_size=training_args_data.get('batchSize', 2),\n",
        "                    gradient_accumulation_steps=8,\n",
        "                    learning_rate=training_args_data.get('learningRate', 2e-4),\n",
        "                    bf16=True,\n",
        "                    logging_steps=10,\n",
        "                    save_steps=100,\n",
        "                    save_total_limit=2,\n",
        "                    optim=\"paged_adamw_8bit\",\n",
        "                    warmup_steps=10,\n",
        "                )\n",
        "                \n",
        "                # Trainer\n",
        "                trainer = Trainer(\n",
        "                    model=model,\n",
        "                    args=training_args,\n",
        "                    train_dataset=tokenized_dataset,\n",
        "                )\n",
        "                \n",
        "                print(\"üöÄ Iniciando treino LoRA...\")\n",
        "                \n",
        "                # TREINAR!\n",
        "                trainer.train()\n",
        "                \n",
        "                print(\"‚úÖ Treino conclu√≠do!\")\n",
        "                \n",
        "                # Salvar adapter\n",
        "                adapter_path = f\"{output_dir}/adapter\"\n",
        "                model.save_pretrained(adapter_path)\n",
        "                \n",
        "                print(f\"üíæ Adapter salvo em: {adapter_path}\")\n",
        "                \n",
        "                # Atualizar estado\n",
        "                state = load_state()\n",
        "                state[\"active_adapter\"] = adapter_path\n",
        "                state[\"last_updated\"] = datetime.now(TZ).isoformat()\n",
        "                save_state(state)\n",
        "                \n",
        "                worker_state[\"status\"] = \"idle\"\n",
        "                worker_state[\"current_job\"] = None\n",
        "                worker_state[\"jobs_completed\"] += 1\n",
        "                \n",
        "                # Limpar cache\n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "                print(f\"\\nüéâ JOB {job_id} CONCLU√çDO COM SUCESSO!\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Erro no treino: {str(e)}\")\n",
        "                worker_state[\"status\"] = \"error\"\n",
        "                worker_state[\"current_job\"] = None\n",
        "        \n",
        "        Thread(target=train_thread, daemon=True).start()\n",
        "        \n",
        "        return jsonify({\n",
        "            \"status\": \"success\",\n",
        "            \"message\": \"Training started\",\n",
        "            \"job_id\": job_id,\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"status\": \"error\", \"message\": str(e)}), 500\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# ENDPOINT: /status\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "@app.route('/status', methods=['GET'])\n",
        "def status():\n",
        "    \"\"\"Status atual do worker\"\"\"\n",
        "    state = load_state()\n",
        "    return jsonify({\n",
        "        **worker_state,\n",
        "        \"active_adapter\": state.get(\"active_adapter\"),\n",
        "        \"last_updated\": state.get(\"last_updated\"),\n",
        "    })\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# AUTO-SHUTDOWN MONITOR\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def auto_shutdown_monitor():\n",
        "    \"\"\"Monitor de auto-shutdown\"\"\"\n",
        "    while True:\n",
        "        runtime_seconds = (datetime.now(TZ) - SESSION_START).total_seconds()\n",
        "        remaining_seconds = (EFFECTIVE_LIMIT_HOURS * 3600) - runtime_seconds\n",
        "        \n",
        "        if remaining_seconds <= 0:\n",
        "            print(\"\\n\" + \"=\" * 70)\n",
        "            print(\"‚è∞ AUTO-SHUTDOWN: Limite de tempo atingido!\")\n",
        "            print(\"=\" * 70)\n",
        "            print(f\"Runtime: {runtime_seconds/3600:.2f}h\")\n",
        "            print(f\"Limite: {EFFECTIVE_LIMIT_HOURS}h\")\n",
        "            print(\"\\nüõë Desligando worker...\")\n",
        "            \n",
        "            try:\n",
        "                requests.post(\n",
        "                    f\"{AION_URL}/api/gpu/shutdown\",\n",
        "                    json={\"worker_name\": os.environ['WORKER_NAME']},\n",
        "                    timeout=5\n",
        "                )\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "            os.kill(os.getpid(), signal.SIGTERM)\n",
        "            break\n",
        "            \n",
        "        time.sleep(60)\n",
        "\n",
        "Thread(target=auto_shutdown_monitor, daemon=True).start()\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# INICIAR SERVIDOR\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ SERVIDOR GPU WORKER COM TREINO E INFER√äNCIA REAIS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üåç URL: {os.environ['WORKER_URL']}\")\n",
        "print(f\"üéÆ Device: {device} ({gpu_name})\")\n",
        "print(f\"ü§ñ Modelo base: {BASE_MODEL}\")\n",
        "print(f\"‚è∞ Auto-shutdown: {EFFECTIVE_LIMIT_HOURS}h\")\n",
        "print(\"\")\n",
        "print(\"‚úÖ Endpoints REAIS:\")\n",
        "print(\"   /health - Health check\")\n",
        "print(\"   /train - LoRA fine-tuning REAL\")\n",
        "print(\"   /inference - Infer√™ncia REAL com modelos treinados\")\n",
        "print(\"   /status - Status do worker\")\n",
        "print(\"\")\n",
        "print(\"‚ö†Ô∏è  N√ÉO FECHE ESTA ABA!\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\")\n",
        "\n",
        "app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}