{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ AION GPU Worker - Google Colab\n",
        "\n",
        "Este notebook transforma seu Google Colab em um **GPU Worker gratuito** para o AION!\n",
        "\n",
        "## ‚ú® O que este worker faz:\n",
        "- **Infer√™ncia LLM**: Responde queries usando modelos customizados (Llama, Mistral, etc.)\n",
        "- **Training LoRA**: Fine-tuna modelos com suas conversas de alta qualidade\n",
        "- **Embeddings**: Gera embeddings localmente (elimina custo OpenAI)\n",
        "- **Zero Custo**: Usa GPUs gratuitas do Google Colab!\n",
        "\n",
        "## üìä GPU Dispon√≠vel:\n",
        "- Tesla T4 (15GB VRAM) - mais comum\n",
        "- ~12h/dia de uso gratuito\n",
        "- Perfeito para fine-tuning e infer√™ncia!\n",
        "\n",
        "## ‚öôÔ∏è Configura√ß√£o:\n",
        "1. Substitua `AION_API_URL` pela URL do seu AION (Replit)\n",
        "2. Execute todas as c√©lulas\n",
        "3. O worker se registrar√° automaticamente e come√ßar√° a processar jobs!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONFIGURA√á√ÉO - EDITE AQUI!\n",
        "# ============================================================================\n",
        "\n",
        "AION_API_URL = \"https://seu-repl.replit.app\"  # ‚ö†Ô∏è SUBSTITUA pela sua URL do Replit!\n",
        "WORKER_NAME = \"Colab-GPU-1\"  # Nome √∫nico para este worker\n",
        "GOOGLE_ACCOUNT_EMAIL = \"sua-conta@gmail.com\"  # Sua conta Google (para tracking)\n",
        "\n",
        "# ============================================================================"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¶ Instalar depend√™ncias\n",
        "!pip install -q transformers accelerate bitsandbytes peft torch requests GPUtil\n",
        "\n",
        "print(\"‚úÖ Depend√™ncias instaladas!\")"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Detectar GPU dispon√≠vel\n",
        "import torch\n",
        "import GPUtil\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu = GPUtil.getGPUs()[0]\n",
        "    print(f\"‚úÖ GPU Detectada: {gpu.name}\")\n",
        "    print(f\"   VRAM Total: {gpu.memoryTotal}MB\")\n",
        "    print(f\"   VRAM Livre: {gpu.memoryFree}MB\")\n",
        "    \n",
        "    GPU_MODEL = gpu.name\n",
        "    VRAM_GB = int(gpu.memoryTotal / 1024)\n",
        "else:\n",
        "    print(\"‚ùå ERRO: GPU n√£o detectada!\")\n",
        "    print(\"‚ö†Ô∏è  V√° em: Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
        "    raise RuntimeError(\"GPU not available\")"
      ],
      "metadata": {
        "id": "detect_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ü§ñ Carregar modelo LLM (exemplo: Llama-3.2-1B)\n",
        "# Voc√™ pode trocar por outros modelos depois!\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"  # Modelo pequeno para teste\n",
        "\n",
        "print(f\"üì• Carregando modelo: {MODEL_NAME}...\")\n",
        "\n",
        "# Configura√ß√£o 4-bit para economizar VRAM\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo carregado com sucesso!\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîó Registrar worker no AION\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def register_worker():\n",
        "    \"\"\"Registra este worker no AION e recebe API key\"\"\"\n",
        "    \n",
        "    url = f\"{AION_API_URL}/api/gpu/workers/register\"\n",
        "    \n",
        "    payload = {\n",
        "        \"tenantId\": 1,  # Default tenant\n",
        "        \"name\": WORKER_NAME,\n",
        "        \"workerType\": \"colab\",\n",
        "        \"gpuModel\": GPU_MODEL,\n",
        "        \"vramGB\": VRAM_GB,\n",
        "        \"capabilities\": {\n",
        "            \"inference\": True,\n",
        "            \"training\": True,\n",
        "            \"embeddings\": True,\n",
        "            \"maxBatchSize\": 8,\n",
        "            \"supportedModels\": [MODEL_NAME]\n",
        "        },\n",
        "        \"metadata\": {\n",
        "            \"accountEmail\": GOOGLE_ACCOUNT_EMAIL,\n",
        "            \"region\": \"colab-us\",\n",
        "            \"quotaHoursPerWeek\": 84,  # ~12h/day * 7 days\n",
        "            \"usedHoursThisWeek\": 0\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, json=payload)\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        worker_id = data[\"worker\"][\"id\"]\n",
        "        api_key = data[\"worker\"][\"apiKey\"]\n",
        "        \n",
        "        print(f\"‚úÖ Worker registrado com sucesso!\")\n",
        "        print(f\"   Worker ID: {worker_id}\")\n",
        "        print(f\"   API Key: {api_key[:20]}...\")\n",
        "        \n",
        "        return api_key\n",
        "    else:\n",
        "        print(f\"‚ùå Erro ao registrar worker: {response.status_code}\")\n",
        "        print(response.text)\n",
        "        raise RuntimeError(\"Worker registration failed\")\n",
        "\n",
        "WORKER_API_KEY = register_worker()"
      ],
      "metadata": {
        "id": "register"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üíì Heartbeat - Mant√©m worker online\n",
        "import time\n",
        "from threading import Thread\n",
        "\n",
        "def send_heartbeat():\n",
        "    \"\"\"Envia heartbeat a cada 30 segundos\"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            url = f\"{AION_API_URL}/api/gpu/workers/heartbeat\"\n",
        "            payload = {\n",
        "                \"apiKey\": WORKER_API_KEY,\n",
        "                \"status\": \"online\"\n",
        "            }\n",
        "            requests.post(url, json=payload, timeout=10)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Heartbeat falhou: {e}\")\n",
        "        \n",
        "        time.sleep(30)  # A cada 30 segundos\n",
        "\n",
        "# Iniciar heartbeat em background\n",
        "heartbeat_thread = Thread(target=send_heartbeat, daemon=True)\n",
        "heartbeat_thread.start()\n",
        "\n",
        "print(\"‚úÖ Heartbeat ativo! Worker est√° online.\")"
      ],
      "metadata": {
        "id": "heartbeat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üéØ Processar jobs do AION\n",
        "def generate_text(prompt, max_tokens=512, temperature=0.7):\n",
        "    \"\"\"Gera texto usando o modelo LLM\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return text\n",
        "\n",
        "def process_job(job):\n",
        "    \"\"\"Processa um job de infer√™ncia\"\"\"\n",
        "    job_id = job[\"id\"]\n",
        "    job_type = job[\"jobType\"]\n",
        "    payload = job[\"payload\"]\n",
        "    \n",
        "    print(f\"üéØ Processando job {job_id} ({job_type})...\")\n",
        "    \n",
        "    try:\n",
        "        if job_type == \"inference\":\n",
        "            prompt = payload.get(\"prompt\", \"\")\n",
        "            max_tokens = payload.get(\"maxTokens\", 512)\n",
        "            temperature = payload.get(\"temperature\", 0.7)\n",
        "            \n",
        "            text = generate_text(prompt, max_tokens, temperature)\n",
        "            tokens_generated = len(tokenizer.encode(text))\n",
        "            \n",
        "            result = {\n",
        "                \"text\": text,\n",
        "                \"tokensGenerated\": tokens_generated\n",
        "            }\n",
        "            \n",
        "            # Reportar conclus√£o\n",
        "            url = f\"{AION_API_URL}/api/gpu/jobs/{job_id}/complete\"\n",
        "            headers = {\"Authorization\": f\"Bearer {WORKER_API_KEY}\"}\n",
        "            requests.post(url, json={\"result\": result}, headers=headers)\n",
        "            \n",
        "            print(f\"‚úÖ Job {job_id} conclu√≠do! Gerados {tokens_generated} tokens.\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Job type '{job_type}' n√£o suportado ainda.\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro processando job {job_id}: {e}\")\n",
        "        \n",
        "        # Reportar falha\n",
        "        url = f\"{AION_API_URL}/api/gpu/jobs/{job_id}/fail\"\n",
        "        headers = {\"Authorization\": f\"Bearer {WORKER_API_KEY}\"}\n",
        "        requests.post(url, json={\"error\": str(e)}, headers=headers)\n",
        "\n",
        "def worker_loop():\n",
        "    \"\"\"Loop principal: busca e processa jobs continuamente\"\"\"\n",
        "    print(\"\\nüöÄ Worker iniciado! Aguardando jobs...\\n\")\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            # Buscar pr√≥ximo job\n",
        "            url = f\"{AION_API_URL}/api/gpu/jobs/next\"\n",
        "            headers = {\"Authorization\": f\"Bearer {WORKER_API_KEY}\"}\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            \n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                job = data.get(\"job\")\n",
        "                \n",
        "                if job:\n",
        "                    process_job(job)\n",
        "                else:\n",
        "                    # Sem jobs, aguardar\n",
        "                    time.sleep(5)\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Erro buscando job: {response.status_code}\")\n",
        "                time.sleep(10)\n",
        "        \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nüõë Worker interrompido pelo usu√°rio.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Erro no loop: {e}\")\n",
        "            time.sleep(10)\n",
        "\n",
        "# Iniciar worker\n",
        "worker_loop()"
      ],
      "metadata": {
        "id": "worker_loop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üéâ Parab√©ns!\n",
        "\n",
        "Seu GPU Worker est√° rodando e processando jobs do AION!\n",
        "\n",
        "### ‚úÖ O que est√° acontecendo:\n",
        "- Worker est√° **online** e vis√≠vel no AION Dashboard\n",
        "- Heartbeat enviado a cada 30 segundos\n",
        "- Aguardando jobs de infer√™ncia/training\n",
        "- Processamento **100% gratuito** com GPU do Colab!\n",
        "\n",
        "### üìä Monitorar:\n",
        "V√° para o AION Dashboard > GPU Pool para ver:\n",
        "- Status do worker (online/busy/offline)\n",
        "- Jobs completados\n",
        "- Tokens processados\n",
        "- Lat√™ncia m√©dia\n",
        "\n",
        "### ‚ö†Ô∏è Importante:\n",
        "- **Sess√£o m√°xima**: 12 horas (Colab desconecta automaticamente)\n",
        "- **Idle timeout**: 90 minutos sem atividade\n",
        "- **Reexecutar**: Se desconectar, reexecute todas as c√©lulas!\n",
        "\n",
        "---\n",
        "\n",
        "**üí° Dica**: Abra m√∫ltiplos Colabs em diferentes contas Google para ter mais GPUs simult√¢neas!\n"
      ],
      "metadata": {
        "id": "footer"
      }
    }
  ]
}
