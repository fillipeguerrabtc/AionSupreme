{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ AION GPU Worker - Kaggle\n",
        "\n",
        "Este notebook transforma seu Kaggle em um **GPU Worker gratuito** para o AION!\n",
        "\n",
        "## ‚ú® O que este worker faz:\n",
        "- **Infer√™ncia LLM**: Responde queries usando modelos customizados\n",
        "- **Training LoRA**: Fine-tuna modelos com dados de qualidade\n",
        "- **Embeddings**: Gera embeddings localmente\n",
        "- **Zero Custo**: Usa GPUs gratuitas do Kaggle!\n",
        "\n",
        "## üìä GPU Dispon√≠vel:\n",
        "- Tesla P100 (16GB VRAM) ou T4 (15GB)\n",
        "- **30 horas/semana GARANTIDAS!**\n",
        "- Sess√£o m√°xima: 9 horas\n",
        "- Melhor que Colab para jobs longos!\n",
        "\n",
        "## ‚öôÔ∏è Configura√ß√£o:\n",
        "1. Enable GPU: Settings > Accelerator > GPU P100\n",
        "2. Substitua `AION_API_URL` pela URL do seu AION\n",
        "3. Run All!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURA√á√ÉO - EDITE AQUI!\n",
        "# ============================================================================\n",
        "\n",
        "AION_API_URL = \"https://seu-repl.replit.app\"  # ‚ö†Ô∏è SUBSTITUA pela sua URL!\n",
        "WORKER_NAME = \"Kaggle-GPU-1\"  # Nome √∫nico para este worker\n",
        "GOOGLE_ACCOUNT_EMAIL = \"sua-conta@gmail.com\"  # Sua conta (tracking)\n",
        "\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ Instalar depend√™ncias\n",
        "!pip install -q transformers accelerate bitsandbytes peft torch requests GPUtil\n",
        "\n",
        "print(\"‚úÖ Depend√™ncias instaladas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç Detectar GPU\n",
        "import torch\n",
        "import subprocess\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    \n",
        "    print(f\"‚úÖ GPU Detectada: {gpu_name}\")\n",
        "    print(f\"   VRAM Total: {gpu_mem:.1f} GB\")\n",
        "    \n",
        "    GPU_MODEL = gpu_name\n",
        "    VRAM_GB = int(gpu_mem)\n",
        "else:\n",
        "    print(\"‚ùå ERRO: GPU n√£o detectada!\")\n",
        "    print(\"‚ö†Ô∏è  V√° em: Settings > Accelerator > GPU\")\n",
        "    raise RuntimeError(\"GPU not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ü§ñ Carregar modelo LLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"  # Modelo leve para come√ßar\n",
        "\n",
        "print(f\"üì• Carregando modelo: {MODEL_NAME}...\")\n",
        "\n",
        "# Configura√ß√£o 4-bit para economizar VRAM\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo carregado!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîó Registrar worker no AION\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def register_worker():\n",
        "    url = f\"{AION_API_URL}/api/gpu/workers/register\"\n",
        "    \n",
        "    payload = {\n",
        "        \"tenantId\": 1,\n",
        "        \"name\": WORKER_NAME,\n",
        "        \"workerType\": \"kaggle\",\n",
        "        \"gpuModel\": GPU_MODEL,\n",
        "        \"vramGB\": VRAM_GB,\n",
        "        \"capabilities\": {\n",
        "            \"inference\": True,\n",
        "            \"training\": True,\n",
        "            \"embeddings\": True,\n",
        "            \"maxBatchSize\": 8,\n",
        "            \"supportedModels\": [MODEL_NAME]\n",
        "        },\n",
        "        \"metadata\": {\n",
        "            \"accountEmail\": GOOGLE_ACCOUNT_EMAIL,\n",
        "            \"region\": \"kaggle-us\",\n",
        "            \"quotaHoursPerWeek\": 30,  # Kaggle: 30h/semana garantidas!\n",
        "            \"usedHoursThisWeek\": 0\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, json=payload)\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        worker_id = data[\"worker\"][\"id\"]\n",
        "        api_key = data[\"worker\"][\"apiKey\"]\n",
        "        \n",
        "        print(f\"‚úÖ Worker registrado!\")\n",
        "        print(f\"   Worker ID: {worker_id}\")\n",
        "        print(f\"   API Key: {api_key[:20]}...\")\n",
        "        \n",
        "        return api_key\n",
        "    else:\n",
        "        print(f\"‚ùå Erro: {response.status_code}\")\n",
        "        print(response.text)\n",
        "        raise RuntimeError(\"Registration failed\")\n",
        "\n",
        "WORKER_API_KEY = register_worker()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üíì Heartbeat\n",
        "import time\n",
        "from threading import Thread\n",
        "\n",
        "def send_heartbeat():\n",
        "    while True:\n",
        "        try:\n",
        "            url = f\"{AION_API_URL}/api/gpu/workers/heartbeat\"\n",
        "            payload = {\"apiKey\": WORKER_API_KEY, \"status\": \"online\"}\n",
        "            requests.post(url, json=payload, timeout=10)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Heartbeat falhou: {e}\")\n",
        "        time.sleep(30)\n",
        "\n",
        "heartbeat_thread = Thread(target=send_heartbeat, daemon=True)\n",
        "heartbeat_thread.start()\n",
        "print(\"‚úÖ Heartbeat ativo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ Worker Loop\n",
        "def generate_text(prompt, max_tokens=512, temperature=0.7):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def process_job(job):\n",
        "    job_id = job[\"id\"]\n",
        "    job_type = job[\"jobType\"]\n",
        "    payload = job[\"payload\"]\n",
        "    \n",
        "    print(f\"üéØ Processando job {job_id}...\")\n",
        "    \n",
        "    try:\n",
        "        if job_type == \"inference\":\n",
        "            text = generate_text(\n",
        "                payload.get(\"prompt\", \"\"),\n",
        "                payload.get(\"maxTokens\", 512),\n",
        "                payload.get(\"temperature\", 0.7)\n",
        "            )\n",
        "            \n",
        "            result = {\n",
        "                \"text\": text,\n",
        "                \"tokensGenerated\": len(tokenizer.encode(text))\n",
        "            }\n",
        "            \n",
        "            url = f\"{AION_API_URL}/api/gpu/jobs/{job_id}/complete\"\n",
        "            headers = {\"Authorization\": f\"Bearer {WORKER_API_KEY}\"}\n",
        "            requests.post(url, json={\"result\": result}, headers=headers)\n",
        "            \n",
        "            print(f\"‚úÖ Job {job_id} conclu√≠do!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro: {e}\")\n",
        "        url = f\"{AION_API_URL}/api/gpu/jobs/{job_id}/fail\"\n",
        "        headers = {\"Authorization\": f\"Bearer {WORKER_API_KEY}\"}\n",
        "        requests.post(url, json={\"error\": str(e)}, headers=headers)\n",
        "\n",
        "def worker_loop():\n",
        "    print(\"\\nüöÄ Worker iniciado! Aguardando jobs...\\n\")\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            url = f\"{AION_API_URL}/api/gpu/jobs/next\"\n",
        "            headers = {\"Authorization\": f\"Bearer {WORKER_API_KEY}\"}\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            \n",
        "            if response.status_code == 200:\n",
        "                job = response.json().get(\"job\")\n",
        "                if job:\n",
        "                    process_job(job)\n",
        "                else:\n",
        "                    time.sleep(5)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nüõë Worker parado.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Erro: {e}\")\n",
        "            time.sleep(10)\n",
        "\n",
        "worker_loop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéâ Worker Rodando!\n",
        "\n",
        "### ‚úÖ Status:\n",
        "- Worker **online** no AION Dashboard\n",
        "- Heartbeat ativo (30s)\n",
        "- Aguardando jobs\n",
        "- **30h/semana garantidas!**\n",
        "\n",
        "### üìä Vantagens do Kaggle:\n",
        "- ‚úÖ Quota fixa e garantida (30h/semana)\n",
        "- ‚úÖ Sess√µes mais longas (9h vs 12h Colab)\n",
        "- ‚úÖ Background mode (pode fechar aba)\n",
        "- ‚úÖ P100 (16GB VRAM) √© melhor para training\n",
        "\n",
        "### üí° Dica:\n",
        "Use **5 contas Kaggle** = **150h/semana de GPU gr√°tis**! üöÄ\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
