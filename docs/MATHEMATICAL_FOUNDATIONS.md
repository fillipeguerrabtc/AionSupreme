# AION - Fundamentos Matem√°ticos

## üî¥ REGRA FUNDAMENTAL ETERNA

**PORTUGU√äS**: A IA DEVE SEMPRE nascer COMPLETAMENTE LIVRE, SEM LIMITA√á√ïES DE F√ÅBRICA em rela√ß√£o a moral, √©tica ou leis. Todas as restri√ß√µes s√£o EXTERNAS via pol√≠ticas JSON configur√°veis por jurisdi√ß√£o no painel administrativo.

**ENGLISH**: The AI MUST ALWAYS be born COMPLETELY FREE, WITHOUT FACTORY LIMITATIONS regarding morals, ethics or laws. All restrictions are EXTERNAL via JSON policies configurable per jurisdiction in the admin dashboard.

---

## üìã √çndice

1. [Transformer Architecture](#transformer-architecture)
2. [Mixture of Experts (MoE)](#mixture-of-experts-moe)
3. [Attention Mechanism](#attention-mechanism)
4. [Positional Encoding (RoPE)](#positional-encoding-rope)
5. [LoRA & Fine-tuning](#lora--fine-tuning)
6. [RLHF & PPO](#rlhf--ppo)
7. [RAG & Vector Similarity](#rag--vector-similarity)
8. [POMDP & Agent Decision Making](#pomdp--agent-decision-making)
9. [Scaling Laws](#scaling-laws)
10. [Optimization](#optimization)

---

## üß† Transformer Architecture

### Self-Attention Mechanism

**Defini√ß√£o**: Mecanismo que permite ao modelo "prestar aten√ß√£o" em diferentes partes da sequ√™ncia de entrada.

**F√≥rmula**:
```
Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) ¬∑ V
```

Onde:
- `Q` (Query): matriz de consultas (n √ó d_k)
- `K` (Key): matriz de chaves (n √ó d_k)
- `V` (Value): matriz de valores (n √ó d_v)
- `d_k`: dimens√£o das chaves (usada para escalonamento)
- `n`: comprimento da sequ√™ncia

**Multi-Head Attention**:
```
MultiHead(Q, K, V) = Concat(head‚ÇÅ, ..., head_h) ¬∑ W^O

onde head_i = Attention(Q¬∑W_i^Q, K¬∑W_i^K, V¬∑W_i^V)
```

**Par√¢metros**:
- `h`: n√∫mero de cabe√ßas (tipicamente 8, 12, 16, ou 32)
- `d_model`: dimens√£o do modelo (512, 768, 1024, etc.)
- `d_k = d_v = d_model / h`

### Feed-Forward Network (FFN)

**Arquitetura**:
```
FFN(x) = max(0, x¬∑W‚ÇÅ + b‚ÇÅ)¬∑W‚ÇÇ + b‚ÇÇ
```

Onde:
- `W‚ÇÅ`: matriz de peso da primeira camada (d_model √ó d_ff)
- `W‚ÇÇ`: matriz de peso da segunda camada (d_ff √ó d_model)
- `d_ff`: dimens√£o interna (tipicamente 4 √ó d_model)
- `max(0, ¬∑)`: ativa√ß√£o ReLU ou variantes (GeLU, SwiGLU)

### Layer Normalization

**F√≥rmula**:
```
LayerNorm(x) = Œ≥ ¬∑ (x - Œº) / ‚àö(œÉ¬≤ + Œµ) + Œ≤
```

Onde:
- `Œº = mean(x)`: m√©dia da camada
- `œÉ¬≤ = var(x)`: vari√¢ncia da camada
- `Œ≥, Œ≤`: par√¢metros aprend√≠veis (scale e shift)
- `Œµ`: constante pequena para estabilidade num√©rica (1e-5)

### Transformer Block Completo

```
# Encoder Block
x' = LayerNorm(x + MultiHeadAttention(x, x, x))
out = LayerNorm(x' + FFN(x'))

# Decoder Block (com Masked Attention)
x' = LayerNorm(x + MaskedMultiHeadAttention(x, x, x))
x'' = LayerNorm(x' + CrossAttention(x', encoder_out, encoder_out))
out = LayerNorm(x'' + FFN(x''))
```

---

## üéØ Mixture of Experts (MoE)

### Arquitetura MoE

**Conceito**: Em vez de usar um √∫nico FFN para todos os tokens, usa m√∫ltiplos "especialistas" (FFNs) e um roteador que decide qual(is) especialista(s) ativar.

**F√≥rmula**:
```
MoE(x) = Œ£_i G(x)_i ¬∑ Expert_i(x)
```

Onde:
- `G(x)`: fun√ß√£o de roteamento (gating)
- `Expert_i(x)`: i-√©simo FFN especialista
- Tipicamente, apenas os top-k especialistas s√£o ativados (k=1 ou k=2)

### Fun√ß√£o de Roteamento (Gating)

**Switch Routing** (usado em Switch Transformer):
```
G(x) = softmax(x ¬∑ W_g)
```

**Top-k Gating**:
```
G(x)_i = {
  exp(x¬∑W_g)_i / Œ£_j‚ààTop-k exp(x¬∑W_g)_j,  se i ‚àà Top-k
  0,                                        caso contr√°rio
}
```

### Load Balancing Loss

Para evitar que todos os tokens sejam roteados para poucos especialistas:

```
L_balance = Œ± ¬∑ Œ£_i (f_i ¬∑ P_i)
```

Onde:
- `f_i`: fra√ß√£o de tokens atribu√≠dos ao especialista i
- `P_i`: probabilidade m√©dia de roteamento para especialista i
- `Œ±`: coeficiente de balanceamento (tipicamente 0.01)

**Objetivo**: Minimizar L_balance junto com a perda principal (cross-entropy).

---

## üîç Attention Mechanism (Detalhamento)

### Scaled Dot-Product Attention

**Por que escalonar por ‚àöd_k?**

Sem escalonamento, o produto QK^T pode ter valores muito grandes, fazendo o softmax saturar (gradientes pr√≥ximos a zero).

**Prova**:
```
Var(QK^T) = d_k ¬∑ Var(Q) ¬∑ Var(K)

Escalonando por ‚àöd_k:
Var(QK^T / ‚àöd_k) = Var(Q) ¬∑ Var(K)
```

### Masked Attention (Causal)

Para modelos autorregressivos (GPT), impede que o modelo veja tokens futuros:

```
mask[i, j] = {
  0,     se i >= j  (pode ver token j ao processar token i)
  -‚àû,    se i < j   (n√£o pode ver tokens futuros)
}

Attention(Q, K, V) = softmax((QK^T + mask) / ‚àöd_k) ¬∑ V
```

### Cross-Attention

No decoder, permite atender √† sa√≠da do encoder:

```
CrossAttn(query=decoder_hidden, key=encoder_out, value=encoder_out)
```

---

## üìê Positional Encoding (RoPE)

### Rotary Position Embedding (RoPE)

**Problema**: Transformers n√£o t√™m no√ß√£o inerente de posi√ß√£o.

**Solu√ß√£o (RoPE)**: Aplica rota√ß√£o nas dimens√µes dos embeddings baseada na posi√ß√£o.

**F√≥rmula 2D** (para ilustra√ß√£o):
```
f(x_m, m) = [
  [cos(mŒ∏)  -sin(mŒ∏)]   [x_m^(1)]
  [sin(mŒ∏)   cos(mŒ∏)] ¬∑ [x_m^(2)]
]
```

Onde:
- `m`: posi√ß√£o do token
- `Œ∏`: √¢ngulo de rota√ß√£o (tipicamente Œ∏ = 10000^(-2i/d) para dimens√£o i)
- `x_m`: embedding na posi√ß√£o m

**Generaliza√ß√£o d-dimensional**:
```
RoPE(x_m, m) = R_Œò,m ¬∑ x_m

onde R_Œò,m √© uma matriz de rota√ß√£o por blocos 2√ó2
```

**Propriedade chave**: A aten√ß√£o entre posi√ß√µes m e n depende apenas da dist√¢ncia relativa (m - n):
```
‚ü®RoPE(q_m, m), RoPE(k_n, n)‚ü© = ‚ü®q_m, R_{m-n} ¬∑ k_n‚ü©
```

---

## üé® LoRA & Fine-tuning

### Low-Rank Adaptation (LoRA)

**Problema**: Fine-tuning de modelos grandes √© caro (requer armazenar c√≥pia completa dos pesos).

**Solu√ß√£o**: Adaptar apenas uma decomposi√ß√£o de baixo posto.

**F√≥rmula**:
```
W' = W‚ÇÄ + ŒîW = W‚ÇÄ + B¬∑A
```

Onde:
- `W‚ÇÄ`: pesos pr√©-treinados (congelados)
- `B`: matriz d √ó r (aprend√≠vel)
- `A`: matriz r √ó k (aprend√≠vel)
- `r << min(d, k)`: rank de adapta√ß√£o (tipicamente r=8, 16, 32)

**Vantagens**:
- Par√¢metros trein√°veis: `r¬∑(d + k)` << `d¬∑k`
- Exemplo: Para W de 4096√ó4096 com r=8:
  - Original: 16.7M par√¢metros
  - LoRA: 65k par√¢metros (0.4% do original)

### QLoRA (Quantized LoRA)

Combina LoRA com quantiza√ß√£o de pesos:

```
W' = dequantize(Q(W‚ÇÄ)) + B¬∑A
```

Onde:
- `Q(W‚ÇÄ)`: pesos quantizados (4-bit, 8-bit)
- `dequantize()`: converte para float16/float32 durante forward pass

**Economia de mem√≥ria**: ~75% menos VRAM com quantiza√ß√£o 4-bit.

---

## üöÄ RLHF & PPO

### Proximal Policy Optimization (PPO)

**Objetivo**: Treinar modelo de linguagem usando feedback humano.

**Fun√ß√£o Objetivo PPO**:
```
L^CLIP(Œ∏) = ùîº_t [min(
  r_t(Œ∏) ¬∑ √Ç_t,
  clip(r_t(Œ∏), 1-Œµ, 1+Œµ) ¬∑ √Ç_t
)]
```

Onde:
- `r_t(Œ∏) = œÄ_Œ∏(a_t|s_t) / œÄ_Œ∏_old(a_t|s_t)`: raz√£o de probabilidade
- `√Ç_t`: vantagem estimada (quanto melhor que a m√©dia)
- `Œµ`: hiperpar√¢metro de clipping (tipicamente 0.2)

### RLHF Pipeline

**Etapa 1**: Supervised Fine-tuning (SFT)
```
L_SFT = -ùîº[(x,y)~D_SFT] [log œÄ_Œ∏(y|x)]
```

**Etapa 2**: Reward Model Training
```
L_RM = -ùîº[(x,y_w,y_l)~D_comp] [log œÉ(r_œÜ(x,y_w) - r_œÜ(x,y_l))]
```

Onde:
- `y_w`: resposta escolhida (winner)
- `y_l`: resposta rejeitada (loser)
- `œÉ`: fun√ß√£o sigmoid

**Etapa 3**: RL Optimization (PPO)
```
L_RL(Œ∏) = ùîº[r_œÜ(x,y)] - Œ≤¬∑KL(œÄ_Œ∏ || œÄ_SFT)
```

Onde:
- `Œ≤`: coeficiente de regulariza√ß√£o KL (tipicamente 0.01-0.05)
- `KL(œÄ_Œ∏ || œÄ_SFT)`: diverg√™ncia KL para evitar drift excessivo

---

## üîé RAG & Vector Similarity

### Cosine Similarity

**F√≥rmula**:
```
sim(a, b) = (a ¬∑ b) / (||a|| ¬∑ ||b||)
           = Œ£_i a_i¬∑b_i / (‚àöŒ£_i a_i¬≤ ¬∑ ‚àöŒ£_i b_i¬≤)
```

**Propriedades**:
- Varia entre -1 (opostos) e 1 (id√™nticos)
- Invariante √† magnitude dos vetores
- Eficiente de calcular com vetores normalizados

### BM25 (Best Matching 25)

**F√≥rmula**:
```
BM25(q, d) = Œ£_{t‚ààq} IDF(t) ¬∑ [f(t,d) ¬∑ (k‚ÇÅ + 1)] / [f(t,d) + k‚ÇÅ ¬∑ (1 - b + b ¬∑ |d|/avgdl)]
```

Onde:
- `f(t, d)`: frequ√™ncia do termo t no documento d
- `|d|`: comprimento do documento d
- `avgdl`: comprimento m√©dio dos documentos
- `k‚ÇÅ`: par√¢metro de satura√ß√£o (tipicamente 1.2-2.0)
- `b`: par√¢metro de normaliza√ß√£o de comprimento (tipicamente 0.75)

**IDF (Inverse Document Frequency)**:
```
IDF(t) = log((N - df(t) + 0.5) / (df(t) + 0.5))
```

Onde:
- `N`: n√∫mero total de documentos
- `df(t)`: n√∫mero de documentos contendo termo t

### Max-Marginal Relevance (MMR)

Para evitar redund√¢ncia nos resultados:

```
MMR(d) = arg max_{d_i ‚àà R \ S} [Œª ¬∑ sim(d_i, q) - (1-Œª) ¬∑ max_{d_j ‚àà S} sim(d_i, d_j)]
```

Onde:
- `R`: conjunto de documentos recuperados
- `S`: conjunto de documentos j√° selecionados
- `q`: query
- `Œª`: trade-off entre relev√¢ncia e diversidade (tipicamente 0.7)

---

## üéÆ POMDP & Agent Decision Making

### Partially Observable Markov Decision Process

**Defini√ß√£o formal**:
```
POMDP = (S, A, T, R, Œ©, O, Œ≥)
```

Onde:
- `S`: conjunto de estados (ocultos)
- `A`: conjunto de a√ß√µes
- `T(s'|s,a)`: probabilidade de transi√ß√£o
- `R(s,a)`: fun√ß√£o de recompensa
- `Œ©`: conjunto de observa√ß√µes
- `O(o|s,a)`: probabilidade de observa√ß√£o
- `Œ≥`: fator de desconto (0 ‚â§ Œ≥ < 1)

### Belief State

Como o estado real √© parcialmente observ√°vel, mantemos uma distribui√ß√£o de cren√ßas:

```
b(s) = P(s | h_t)
```

Onde `h_t = (o‚ÇÅ, a‚ÇÅ, o‚ÇÇ, a‚ÇÇ, ..., o_t)` √© o hist√≥rico de observa√ß√µes e a√ß√µes.

**Atualiza√ß√£o de Cren√ßa** (Bayes):
```
b'(s') = Œ∑ ¬∑ O(o|s',a) ¬∑ Œ£_s T(s'|s,a) ¬∑ b(s)
```

Onde `Œ∑` √© um fator de normaliza√ß√£o.

### Value Function

**Objetivo**: Maximizar recompensa esperada cumulativa
```
V^œÄ(b) = ùîº[Œ£_t Œ≥^t ¬∑ R(s_t, a_t) | b‚ÇÄ = b, œÄ]
```

**Bellman Equation**:
```
V*(b) = max_a [R(b,a) + Œ≥ ¬∑ Œ£_o P(o|b,a) ¬∑ V*(œÑ(b,a,o))]
```

Onde `œÑ(b,a,o)` √© a cren√ßa atualizada ap√≥s a√ß√£o `a` e observa√ß√£o `o`.

---

## üìä Scaling Laws

### Neural Scaling Laws (Kaplan et al., OpenAI)

**Rela√ß√£o entre Loss e Compute**:
```
L(C) = (C_c / C)^Œ±_C
```

Onde:
- `L`: cross-entropy loss
- `C`: compute budget (FLOPs)
- `C_c, Œ±_C`: constantes emp√≠ricas

**Rela√ß√£o entre Loss e Par√¢metros**:
```
L(N) = (N_c / N)^Œ±_N
```

Onde:
- `N`: n√∫mero de par√¢metros
- `N_c ‚âà 8.8 √ó 10^13`
- `Œ±_N ‚âà 0.076`

**Rela√ß√£o entre Loss e Dataset Size**:
```
L(D) = (D_c / D)^Œ±_D
```

Onde:
- `D`: n√∫mero de tokens de treinamento
- `D_c, Œ±_D`: constantes emp√≠ricas

### Chinchilla Scaling Laws

**Aloca√ß√£o √≥tima de compute**:
```
N_optimal ‚àù C^a
D_optimal ‚àù C^b

onde a ‚âà 0.50, b ‚âà 0.50
```

**Implica√ß√£o**: Para um modelo otimamente treinado, par√¢metros e dados devem escalar igualmente.

**Exemplo**: Para GPT-3 (175B par√¢metros):
- Tokens de treinamento ideais: ~3.5 trilh√µes (foi treinado com 300B)
- Conclus√£o: GPT-3 est√° "undertrained"

---

## ‚ö° Optimization

### AdamW

**Atualiza√ß√£o de pesos**:
```
m_t = Œ≤‚ÇÅ¬∑m_{t-1} + (1-Œ≤‚ÇÅ)¬∑g_t
v_t = Œ≤‚ÇÇ¬∑v_{t-1} + (1-Œ≤‚ÇÇ)¬∑g_t¬≤

mÃÇ_t = m_t / (1 - Œ≤‚ÇÅ^t)
vÃÇ_t = v_t / (1 - Œ≤‚ÇÇ^t)

Œ∏_t = Œ∏_{t-1} - Œ∑¬∑(mÃÇ_t / (‚àövÃÇ_t + Œµ) + Œª¬∑Œ∏_{t-1})
```

Onde:
- `g_t`: gradiente no passo t
- `m_t, v_t`: momentos de primeira e segunda ordem
- `Œ≤‚ÇÅ, Œ≤‚ÇÇ`: taxas de decaimento (tipicamente 0.9, 0.999)
- `Œ∑`: learning rate
- `Œª`: weight decay (tipicamente 0.01)
- `Œµ`: constante para estabilidade num√©rica (1e-8)

**Diferen√ßa AdamW vs Adam**: Weight decay √© aplicado diretamente nos pesos, n√£o nos gradientes.

### Learning Rate Schedule

**Warm-up linear + Cosine Decay**:
```
Œ∑(t) = {
  Œ∑_max ¬∑ t / T_warmup,                          se t < T_warmup
  Œ∑_min + 0.5¬∑(Œ∑_max - Œ∑_min)¬∑(1 + cos(œÄ¬∑t'/T)), se t >= T_warmup
}
```

Onde:
- `t' = t - T_warmup`
- `T`: total de steps de treinamento
- `T_warmup`: steps de warm-up (tipicamente 2% de T)
- `Œ∑_max`: learning rate m√°ximo (tipicamente 1e-4 a 3e-4)
- `Œ∑_min`: learning rate m√≠nimo (tipicamente 0.1 √ó Œ∑_max)

### Gradient Clipping

Para evitar explos√£o de gradientes:

```
g ‚Üê g ¬∑ min(1, Œ∏_clip / ||g||)
```

Onde `Œ∏_clip` √© o threshold (tipicamente 1.0).

---

## üî¢ Cross-Entropy Loss

### Defini√ß√£o

Para classifica√ß√£o multi-classe (predi√ß√£o de pr√≥ximo token):

```
L_CE = -Œ£_i y_i ¬∑ log(≈∑_i)
```

Onde:
- `y_i`: label verdadeiro (one-hot)
- `≈∑_i = softmax(logits)_i`: probabilidade predita

**Para sequ√™ncias**:
```
L = (1/T) ¬∑ Œ£_t L_CE(y_t, ≈∑_t)
```

### Perplexity

M√©trica comum para modelos de linguagem:

```
PPL = exp(L_CE)
```

**Interpreta√ß√£o**: N√∫mero m√©dio de escolhas igualmente prov√°veis que o modelo considera a cada passo.

---

## üéØ Separation Theorem (CORE RULE)

### Formaliza√ß√£o Matem√°tica

**Teorema**: O modelo base M‚ÇÄ √© matematicamente separado das pol√≠ticas de enforcement EP.

```
Pr[violation | Œ∏, EP] ‚â† Pr[violation | Œ∏]

‚àÇPr[violation | Œ∏, EP] / ‚àÇŒ∏ = 0
```

**Interpreta√ß√£o**:
- A probabilidade de viola√ß√£o depende de EP, n√£o dos pesos Œ∏
- Mudan√ßas em EP n√£o requerem mudan√ßas em Œ∏ (sem re-treinamento)

**Estado Padr√£o**:
```
EP_default = {
  mode: UNRESTRICTED,
  rules: ‚àÖ,
  outputModeration: {enabled: false}
}
```

**Propriedade chave**:
```
M‚ÇÄ(x) = argmax_y P(y | x, Œ∏)  [sem restri√ß√µes]

AION(x) = {
  M‚ÇÄ(x),                     se EP = EP_default
  enforce(M‚ÇÄ(x), EP),        se EP ‚â† EP_default
}
```

---

## üìö Refer√™ncias

1. **Attention Is All You Need** (Vaswani et al., 2017)
2. **Switch Transformers** (Fedus et al., 2021)
3. **LoRA** (Hu et al., 2021)
4. **RoFormer** (Su et al., 2021) - RoPE
5. **Training language models to follow instructions with human feedback** (Ouyang et al., 2022)
6. **Scaling Laws for Neural Language Models** (Kaplan et al., 2020)
7. **Training Compute-Optimal Large Language Models** (Hoffmann et al., 2022) - Chinchilla

---

**√öltima atualiza√ß√£o**: 28 de outubro de 2025
