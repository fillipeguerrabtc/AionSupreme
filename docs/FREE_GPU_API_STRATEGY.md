# üöÄ Plano para Executar AION Supreme 100% Gratuitamente

**Fonte**: GPU&API_1761707927196.txt (Documenta√ß√£o completa de infraestrutura gratuita)

---

## üìã √çndice

1. [Infraestrutura Gratuita Dispon√≠vel](#infraestrutura-gratuita-dispon√≠vel)
2. [Estrat√©gia para Treinar o Modelo do Zero](#estrat√©gia-para-treinar-o-modelo-do-zero)
3. [Altern√¢ncia Entre Ferramentas Cloud/GPU](#altern√¢ncia-entre-ferramentas-cloudgpu)
4. [Integra√ß√£o com APIs de IA Gratuitas](#integra√ß√£o-com-apis-de-ia-gratuitas)
5. [Adapta√ß√£o do C√≥digo em Plataformas Gratuitas](#adapta√ß√£o-do-c√≥digo-em-plataformas-gratuitas)
6. [Modelos Open-Source Adequados](#modelos-open-source-adequados)

---

## 1. Infraestrutura Gratuita Dispon√≠vel

### Google Colab (Free)
- **GPU**: NVIDIA T4 (~16GB VRAM)
- **Sess√£o**: ~12 horas cont√≠nuas
- **Desconex√£o**: Ap√≥s ~90 min de inatividade
- **Persist√™ncia**: Montar Google Drive para salvar modelos/checkpoints
- **Configura√ß√£o**:
```python
from google.colab import drive
drive.mount('/content/drive')
```

### Google Cloud Platform (Free Tier)
- **VM Always-Free**: e2-micro Linux (CPU) - Permanente
- **Storage**: 5 GiB Cloud Storage gr√°tis
- **Cr√©ditos**: $300 para novos usu√°rios (GPU tempor√°ria)
- **Uso**: Orquestrador ou servidor leve (FastAPI, agendador)

### Kaggle Notebooks
- **GPU**: Nvidia gratuita
- **Limite**: 30 horas/semana
- **Sess√£o**: ~9 horas cont√≠nuas
- **‚ö†Ô∏è Limita√ß√£o**: Notebooks agendados n√£o suportam GPU (apenas CPU)
- **Uso**: Recurso alternativo manual

### Replit
- **Tipo**: Plataforma de hosting cont√≠nuo
- **GPU**: ‚ùå N√£o oferece GPU no plano gr√°tis
- **Uso**: Frontend React, Backend Node.js, Orquestrador
- **Continuidade**: Usar ping peri√≥dico (UptimeRobot a cada 5 min) para evitar hiberna√ß√£o

### APIs de Infer√™ncia com GPU (Gratuitas)

#### Hugging Face Inference API
- **Modelos**: Milhares de modelos open-source
- **GPU**: Nos servidores da HF (gr√°tis para baixo volume)
- **Uso**: Chamadas REST sem hospedar localmente

#### OpenRouter
- **Agregador**: 50+ modelos (Claude, Llama, Mistral)
- **Cr√©ditos**: Gr√°tis no cadastro
- **Modelos gratuitos**: Dispon√≠veis para teste
- **API**: Unificada (trocar modelo mudando par√¢metro)

#### Groq API
- **Modelos**: Llama 3, Mistral (open-source)
- **Hardware**: Especializado (ultrarr√°pido)
- **Limite gratuito**: ~14,400 requisi√ß√µes/dia
- **Uso**: Infer√™ncia em tempo real

---

## 2. Estrat√©gia para Treinar o Modelo do Zero

### üéØ Conceito-Chave: Treinamento em Blocos

**Problema**: Colab tem limite de ~12h por sess√£o  
**Solu√ß√£o**: Treinar em √©pocas curtas + salvar checkpoints

### 2.1. Dividir Treinamento em Sess√µes

```python
# Salvar checkpoint a cada N itera√ß√µes
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/aion/checkpoints",
    save_steps=50,  # Salvar a cada 50 steps
    save_total_limit=3,  # Manter apenas 3 √∫ltimos checkpoints
)
```

**Fluxo**:
1. Treinar por N passos/√©pocas
2. Salvar checkpoint no Google Drive
3. Finalizar sess√£o
4. Nova sess√£o: Carregar checkpoint e continuar

### 2.2. Usar Precis√£o Mista e Modelos Menores

**GPU T4 do Colab**: ~16 GB VRAM

**T√©cnicas de Economia**:
- **FP16** (Half Precision): Reduz uso de VRAM
- **Modelos 7B-13B**: Mais r√°pidos que 175B
- **Quantiza√ß√£o 4-bit**: 16GB ‚Üí 4GB

```python
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="bfloat16",
)
```

### 2.3. Aproveitar M√∫ltiplos Provedores em Paralelo

**Estrat√©gia de Rota√ß√£o**:
```
Seg-Qua: Colab (12h/dia) ‚Üí 36h
Qui-Sex: Kaggle (9h/sess√£o) ‚Üí 18h  
Sab-Dom: Modal ($30 cr√©ditos) ‚Üí ~20h

TOTAL: ~74h GPU/semana (gratuito)
```

**Fluxo**:
1. Colab treina 9-12h ‚Üí Salva checkpoint no Drive
2. Kaggle carrega checkpoint ‚Üí Treina +9h
3. Repete ciclo

### 2.4. Datasets em Partes e Streaming

**Colab Free**: ~12 GB RAM

**Solu√ß√£o**:
```python
from torch.utils.data import DataLoader

# Carregar sob demanda (n√£o tudo na mem√≥ria)
train_loader = DataLoader(
    dataset,
    batch_size=4,
    num_workers=2,
)
```

- Armazenar dados no Google Drive
- Ler em batches conforme necess√°rio
- PyTorch DataLoader com carregamento sob demanda

### 2.5. Monitora√ß√£o e Logging

```python
# TensorBoard.dev (gratuito)
!tensorboard dev upload --logdir ./logs
```

- Log de perda, m√©tricas, amostras
- Salvar logs no Drive
- TensorBoard.dev para visualiza√ß√£o

### 2.6. Teste Cont√≠nuo e Ajuste

**Valida√ß√£o peri√≥dica**:
- Gerar respostas de exemplo
- Avaliar coer√™ncia
- Verificar deriva indesejada

**Curriculum Learning**:
- Come√ßar com dados simples
- Aumentar complexidade gradualmente

---

## 3. Altern√¢ncia Entre Ferramentas Cloud/GPU

### üîÑ Rota√ß√£o de Notebooks (Colab/Kaggle)

**Esquema de Rota√ß√£o**:
```
Manh√£: Colab (salva estado periodicamente)
Tarde: Kaggle (carrega √∫ltimo estado)
Noite: Colab novamente
```

**Estado a Salvar**:
- Vetores de mem√≥ria (FAISS)
- √öltimos di√°logos
- Checkpoints do modelo
- Configura√ß√µes

### ü§ñ Servidor Orquestrador Always-On

**Op√ß√µes**:
1. **VM GCP e2-micro** (always-free)
2. **Replit** (com ping para evitar hiberna√ß√£o)

**Fun√ß√£o**: Watchdog que verifica se inst√¢ncia principal est√° ativa

```typescript
// Pseudo-c√≥digo
async function checkGPUHealth() {
  const response = await fetch('http://ngrok-url.io/health');
  if (!response.ok) {
    // GPU offline, acionar fallback
    await triggerBackupGPU();
  }
}

setInterval(checkGPUHealth, 60000); // A cada 1 min
```

**Solu√ß√µes para Iniciar Colab Automaticamente**:

1. **Google Apps Script + Cloud Scheduler**:
   - Script que abre notebook Colab
   - Automa√ß√£o via Gmail ou similar

2. **Selenium em VM**:
   - Navegador headless na VM always-free
   - Login autom√°tico e abertura do Colab

3. **Alternativa Simples**:
   - Email/SMS quando Colab cair
   - Iniciar manualmente (reduz downtime)

### ‚öñÔ∏è Balanceamento de Carga

**Arquitetura Distribu√≠da**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Replit/GCP (Always-On)              ‚îÇ
‚îÇ ‚Ä¢ Frontend React                     ‚îÇ
‚îÇ ‚Ä¢ Backend FastAPI (CPU)              ‚îÇ
‚îÇ ‚Ä¢ Orquestrador                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì Delega tarefas GPU
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Colab/Kaggle (GPU Tempor√°ria)       ‚îÇ
‚îÇ ‚Ä¢ Modelo de linguagem                ‚îÇ
‚îÇ ‚Ä¢ Infer√™ncia complexa                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Fallback Autom√°tico**:
- GPU offline ‚Üí Usar OpenRouter/HF API
- Frontend nunca sai do ar
- Apenas "motor" de IA muda

### üåê Ngrok e URLs Est√°veis

**Problema**: Ngrok muda URL a cada sess√£o

**Solu√ß√µes**:

1. **Dom√≠nio Gratuito + API de Redirecionamento**:
   - Cloudflare API
   - DynDNS
   - Atualizar subdom√≠nio via script

2. **Frontend Separado**:
   - Hospedar em Replit/Vercel
   - Lista de poss√≠veis URLs de backend
   - Tentar conectar em ordem

3. **Servi√ßo de Discovery**:
   - Orquestrador informa qual backend est√° online

---

## 4. Integra√ß√£o com APIs de IA Gratuitas

### Google AI Studio (Gemini)

**Limites Generosos**:
- **6 milh√µes** de tokens/dia
- **180 milh√µes** de tokens/m√™s

**Integra√ß√£o**:
```python
from google.generativeai import GenerativeModel

model = GenerativeModel('gemini-1.5-flash')
response = model.generate_content(prompt)
```

**Caching**:
- Salvar respostas no Knowledge Base (FAISS)
- Economizar tokens
- Construir base n√£o-censurada

### Hugging Face Inference API

**Modelos N√£o-Censurados**:
- Llama-2 sem filtro
- Mistral 7B Instruct
- Variantes da comunidade

**Uso como Fallback**:
```typescript
if (!localModel.isOnline()) {
  // Chamar HF API
  const response = await hfAPI.inference({
    model: "mistralai/Mistral-7B-Instruct-v0.2",
    inputs: prompt
  });
  
  // Gravar no KB local
  await knowledgeBase.index(response);
}
```

### OpenRouter

**Agregador de 50+ Modelos**:
- Claude, Llama, Mistral, etc.
- Trocar modelo mudando par√¢metro

**Cr√©ditos Gratuitos**: Novos usu√°rios ganham cr√©ditos

**Ensemble**:
```python
# Enviar para 2-3 modelos em paralelo
responses = await Promise.all([
  openrouter.complete({model: "claude-3", prompt}),
  openrouter.complete({model: "llama-3-70b", prompt}),
  openrouter.complete({model: "mistral-large", prompt}),
]);

// Combinar ou escolher melhor
const best = selectBestResponse(responses);
```

### APIs de Imagem/V√≠deo

**Stable Diffusion via HuggingFace**:
```python
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4")
image = pipe(prompt).images[0]
```

**Craiyon API** (antigo DALL-E mini): Gratuito

**Cache de Resultados Visuais**:
- Armazenar no Drive
- CDN gratuita (Imgur via API)

### OpenAI (gpt-3.5-turbo)

**Custo**: Fra√ß√µes de centavo por mil tokens  
**$5**: Milh√µes de tokens

**‚ö†Ô∏è Censura**: OpenAI implementa filtros  
**Uso**: Benchmarks e compara√ß√µes (n√£o fonte de dados n√£o-censurados)

---

## 5. Adapta√ß√£o do C√≥digo em Plataformas Gratuitas

### No Google Colab

#### 5.1. Preparar Ambiente

```python
# Runtime > Change runtime type > GPU
from google.colab import drive
drive.mount('/content/drive')
```

#### 5.2. Instalar Depend√™ncias

```python
!pip install fastapi uvicorn faiss-cpu openai anthropic
!pip install pandas numpy scipy python-multipart
!pip install beautifulsoup4 lxml requests pillow opencv-python-headless
!pip install transformers==4.33.0 accelerate sentence-transformers langchain
```

#### 5.3. Obter C√≥digo

**Op√ß√£o 1 - Clonar GitHub**:
```bash
!git clone https://github.com/fillipeguerrabtc/AionSupreme.git
%cd AionSupreme
```

**Op√ß√£o 2 - Upload ZIP**:
```bash
!unzip /content/AionSupreme-main.zip -d /content/AionSupreme
%cd /content/AionSupreme/AionSupreme-main
```

#### 5.4. Configurar Vari√°veis de Ambiente

```python
import os

# Supabase (PostgreSQL gr√°tis)
os.environ['DATABASE_URL'] = 'postgresql://user:pass@host.supabase.co:5432/postgres'

# APIs (se usar)
os.environ['OPENAI_API_KEY'] = '...'
os.environ['ANTHROPIC_API_KEY'] = '...'
```

**Alternativa**: SQLite local (prot√≥tipo)

#### 5.5. Iniciar Backend FastAPI

```python
!uvicorn app.main:app --host 0.0.0.0 --port 8000 &> log.txt &
```

#### 5.6. Expor com Ngrok

```python
!pip install pyngrok
from pyngrok import ngrok

NGROK_AUTH_TOKEN = "SEU_TOKEN_AQUI"
!ngrok authtoken $NGROK_AUTH_TOKEN

public_url = ngrok.connect(8000, "http")
print(f"üöÄ API dispon√≠vel em: {public_url}")
```

#### 5.7. Frontend React

**Op√ß√£o Recomendada**: Hospedar separadamente em:
- Replit (Node.js)
- Vercel/Netlify (build autom√°tico)

**Configura√ß√£o**:
```javascript
// .env
VITE_API_URL=https://1234.ngrok.io
```

#### 5.8. Salvar Estado Regularmente

```python
# Antes de encerrar sess√£o
model.save_pretrained('/content/drive/MyDrive/aion/model')
faiss_index.save('/content/drive/MyDrive/aion/faiss.index')
```

#### 5.9. Reiniciar Quando Necess√°rio

- Sess√£o expira ‚Üí Montar Drive
- Reinstalar depend√™ncias (ou usar notebook salvo)
- Banco Supabase mant√©m dados
- FAISS recarrega do Drive

### No Replit

#### Criar Repl Python

```bash
# replit.nix ou pip
pip install fastapi uvicorn
```

#### Configurar Secrets

```
DATABASE_URL=postgresql://...
OPENAI_API_KEY=sk-...
```

#### Iniciar FastAPI

```python
# main.py
import uvicorn
from app.main import app

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**URL P√∫blico**: `https://<project>.<username>.repl.co`

#### Manter Ativo

- Frontend rodando ‚Üí Mant√©m acordado
- Ou usar ping externo (UptimeRobot)

### Ajustes no C√≥digo

#### Usar Modelos Locais

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "/content/drive/MyDrive/aion/model",
    load_in_4bit=True,
)
tokenizer = AutoTokenizer.from_pretrained("/content/drive/MyDrive/aion/model")

# Substituir chamadas OpenAI por infer√™ncia local
def generate(prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=512)
    return tokenizer.decode(outputs[0])
```

#### Desabilitar Filtros de Conte√∫do

```typescript
// server/policy-enforcement.ts
const policy = {
  mode: "UNRESTRICTED",
  rules: [],
  outputModeration: { enabled: false }
};
```

---

## 6. Modelos Open-Source Adequados (Sem Censura)

### Llama 2 / Llama 3

**Par√¢metros**: 7B at√© 70B  
**Licen√ßa**: Permissiva (pesquisa)  
**Censura**: Forks n√£o-censurados dispon√≠veis  
**Base**: Sem filtro nativo

**Exemplo**: Llama 3 70B Instruct
- 70B par√¢metros
- Vers√°til (texto + c√≥digo)
- Otimiza√ß√µes avan√ßadas

### Mistral AI

**Mistral 7B**:
- 7B par√¢metros
- C√≥digo aberto (Apache 2.0)
- Performance compar√°vel a modelos maiores
- Sliding Window Attention (contexto longo)

**Mixtral 8x7B**:
- Mixture of Experts
- 47B par√¢metros (8 experts de 7B)
- Qualidade pr√≥xima ao GPT-3.5

### Falcon

**Falcon 40B / 180B**:
- Treinado em 1 trilh√£o+ tokens
- Dados limpos (RefinedWeb)
- Apache 2.0
- Sem censura nativa

### MPT (MosaicML)

**MPT-7B / MPT-30B**:
- Treinado com contexto de 2k-8k tokens
- C√≥digo 100% open-source
- Apache 2.0
- √ìtimo para fine-tuning

### Recomenda√ß√£o: Llama-3-8B + LoRA

**Por qu√™?**:
- ‚úÖ 8B par√¢metros cabem na T4 (4-bit)
- ‚úÖ Base forte (Meta AI)
- ‚úÖ LoRA fine-tuning (~200MB adaptadores)
- ‚úÖ Comunidade ativa (forks n√£o-censurados)
- ‚úÖ Performance excelente

**Matem√°tica LoRA**:
```
W' = W + BA
Onde:
- W = Peso original (congelado)
- B, A = Matrizes de baixo rank (r=16)
- Par√¢metros trein√°veis: ~0.4% do total
```

**Vantagens**:
- Treino r√°pido (horas, n√£o dias)
- Baixo uso de GPU
- Adaptadores pequenos (f√°cil salvar/carregar)
- Combinar m√∫ltiplos LoRAs

---

## üìä Resumo de Limites Gratuitos

| Recurso | Limite | Uso Mensal |
|---------|--------|------------|
| **Colab** | 12h/dia | ~360h GPU |
| **Kaggle** | 30h/semana | ~120h GPU |
| **Modal** | $30 cr√©ditos | ~20-30h GPU |
| **Groq** | 14.4k req/dia | ~432k req |
| **Gemini** | 1.5k req/dia | ~45k req |
| **HF Inference** | ~720 req/dia | ~21.6k req |
| **GCP VM** | e2-micro | 24/7 CPU |
| **Replit** | Ilimitado* | 24/7 CPU |
| **Supabase** | 512MB DB | Ilimitado |

**Total**: ~500h GPU/m√™s + ~500k req LLM/m√™s = **$0**

---

## üéØ Fluxo Completo de Trabalho

```
1. SETUP INICIAL
   ‚îú‚îÄ Criar conta Supabase (PostgreSQL gr√°tis)
   ‚îú‚îÄ Obter API keys (Groq, Gemini, OpenRouter)
   ‚îú‚îÄ Configurar Ngrok
   ‚îî‚îÄ Hospedar frontend em Replit/Vercel

2. TREINAMENTO
   ‚îú‚îÄ Colab: Fine-tune Llama-3-8B + LoRA
   ‚îú‚îÄ Salvar adaptadores no Google Drive
   ‚îî‚îÄ Alternar Colab/Kaggle conforme limites

3. INFER√äNCIA
   ‚îú‚îÄ Colab/Kaggle: FastAPI + modelo local
   ‚îú‚îÄ Ngrok: Expor para Replit
   ‚îî‚îÄ Fallback: Groq ‚Üí Gemini ‚Üí HF

4. ORQUESTRA√á√ÉO
   ‚îú‚îÄ Replit: Always-on (frontend + orquestrador)
   ‚îú‚îÄ Watchdog: Detectar GPU offline
   ‚îî‚îÄ Auto-fallback para APIs gratuitas

5. MANUTEN√á√ÉO
   ‚îú‚îÄ Rota√ß√£o manual de GPUs (seg-dom)
   ‚îú‚îÄ Salvar checkpoints a cada sess√£o
   ‚îî‚îÄ Monitorar via TensorBoard.dev
```

---

## üí° Dicas Avan√ßadas

### 1. Automa√ß√£o com Selenium

```python
from selenium import webdriver

# VM GCP always-free rodando Selenium
driver = webdriver.Chrome()
driver.get("https://colab.research.google.com/")
# Login autom√°tico e abrir notebook
```

### 2. Webhook para Notifica√ß√µes

```typescript
// Notificar quando GPU cair
await fetch('https://hooks.slack.com/...', {
  method: 'POST',
  body: JSON.stringify({
    text: '‚ö†Ô∏è GPU Colab offline! Iniciar Kaggle.'
  })
});
```

### 3. Curriculum Learning

```python
# Treinar do simples ‚Üí complexo
datasets = [
  "simple_conversations.jsonl",  # √âpocas 1-2
  "medium_conversations.jsonl",  # √âpocas 3-5
  "complex_conversations.jsonl", # √âpocas 6-10
]
```

### 4. Ensemble de Modelos

```python
# Combinar respostas de m√∫ltiplas fontes
responses = [
  local_model.generate(prompt),
  groq_api.generate(prompt),
  gemini_api.generate(prompt),
]

best = select_by_confidence(responses)
```

---

## ‚ö†Ô∏è Avisos Importantes

1. **Termos de Servi√ßo**: Respeitar limites de cada plataforma
2. **Contas Secund√°rias**: Verificar se s√£o permitidas
3. **Censura**: Modelos base n√£o t√™m filtro, mas APIs externas (OpenAI) sim
4. **Persist√™ncia**: Sempre salvar no Drive/Storage (Colab/Kaggle s√£o ef√™meros)
5. **Monitoramento**: Logs e checkpoints para rastrear evolu√ß√£o

---

## üöÄ Conclus√£o

Com planejamento cuidadoso, √© **100% vi√°vel** rodar AION Supreme gratuitamente:

‚úÖ **500h GPU/m√™s** (Colab + Kaggle + Modal)  
‚úÖ **500k+ requisi√ß√µes LLM/m√™s** (Groq + Gemini + HF)  
‚úÖ **Always-on frontend** (Replit/Vercel)  
‚úÖ **PostgreSQL persistente** (Supabase)  
‚úÖ **Fine-tuning LoRA** (Llama-3-8B em T4)  
‚úÖ **Fallback autom√°tico** (GPU offline ‚Üí APIs)  

**Custo total**: **$0/m√™s** üéâ

---

**√öltima atualiza√ß√£o**: Outubro 29, 2025  
**Fonte**: GPU&API_1761707927196.txt (208 linhas)
